{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861a59ad-3ba9-486d-beb6-af0ffaf9c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdff956-8f40-4e22-b033-65ea53aa1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign home_dir as a local directory to use when saving data to disk.\n",
    "home_dir = os.getenv(\"home_BA885\")\n",
    "\n",
    "if not home_dir:\n",
    "    home_dir = os.getcwd()\n",
    "# Colab home\n",
    "# home_dir = \"/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6dd29-8873-4dc0-b990-a6457df400d3",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Author: Nima Doroud\n",
    "\n",
    "In the previous notebook, we discussed the embedding of natural language as sequences of vectors in the embedding space. Our next task is to design model architectures that take such sequences as inputs and perform our desired task, be it classification, such as sentiment analysis of movie reviews, or text generation such as machine translation.\n",
    "\n",
    "The subject of this notebook Neural Machine Translation (NMT), a class of Sequence to sequence (Seq2sec) models where both the input and output are sequences of vectors. Specifically, we will focus on (sentence level) Neural Machine Translation (NMT) models capable of translating text from one language (source) to another (target).\n",
    "\n",
    "The most common Seq2seq models adopt the Encoder-Decoder architecture which schematically acts as follows:\n",
    "\n",
    "**Input sequence -> Encoder -> Latent vector -> Decoder -> Output sequence**\n",
    "\n",
    "The task of the encoder is to capture the \"most relevant features\" of the input sequence which it encodes in the latent vector. To enforce this \"distillation\" the latent vector lives in a lower-dimensional space as compared to the input. Meanwhile, the decoder is tasked with generating a new sequence using the latent vector as the prompt.\n",
    "\n",
    "For a machine translation model, we expect the encoder block to capture features such as context/meaning, tense, sentiment, and so on, while ignoring language specific features such as the detailed grammar rules of the source language. The decoder then uses these features to generate a new sentence while adhering to the grammar rules of the target language.\n",
    "\n",
    "The encoder/decoder blocks deal with sequences and thus need to correctly capture both the context and the order of a sequence. Two common architectures designed for this purpose are recurrent neural networks (RNN), in which the ordered nature of the sequence is manifest, and transformer neural networks which utilize \"attention\" to better identify context. Below, we will build both RNN and transformer based NMT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b943be2-3481-4099-9519-66eb3c5771c4",
   "metadata": {},
   "source": [
    "## Bilingual Sentence Pairs\n",
    "\n",
    "We will use a dataset containing pairs of sentences in two languages from <a href=\"https://tatoeba.org/\">Tatoeba Project</a>. You can take your pick from the many pairs of languages they have available, here we will use the Spanish-English dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac24f51-6b2d-4d7c-aca0-758291ef3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "if not os.path.exists(home_dir+'/datasets/spa-eng/spa.txt'):\n",
    "    ds_link = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
    "    tf.keras.utils.get_file('spa-eng.zip', \n",
    "                            ds_link, \n",
    "                            cache_dir='./',\n",
    "                            cache_subdir='datasets',\n",
    "                            extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7d8a5d-a5b0-4769-8ef5-f03fa718bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset from the tab ('\\t') seperated file, make sure to skip the bad lines.\n",
    "df = pd.read_csv(home_dir+'/datasets/spa-eng/spa.txt',\n",
    "                 header = None,\n",
    "                 sep = '\\t',\n",
    "                 on_bad_lines='skip')\n",
    "\n",
    "# Label the columns with English and Spanish sentences as 'en' and 'es', respectively.\n",
    "df = df.rename(columns={0:'en', 1:'es'})\n",
    "\n",
    "# Shuffle the dataset.\n",
    "df = df.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad981f4-c3bd-4df2-8e1c-dae1a51eb23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 118964\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We need to chat soon.</td>\n",
       "      <td>Necesitamos charlar pronto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She married a musician.</td>\n",
       "      <td>Se casó con un músico.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do it right now.</td>\n",
       "      <td>Hazlo ahoritita.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the ticket line.</td>\n",
       "      <td>Esta es la cola para sacar los billetes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         en                                        es\n",
       "0     We need to chat soon.               Necesitamos charlar pronto.\n",
       "1   She married a musician.                    Se casó con un músico.\n",
       "2          Do it right now.                          Hazlo ahoritita.\n",
       "3  This is the ticket line.  Esta es la cola para sacar los billetes."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print('Number of rows = {}'.format(df.shape[0]))\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59afff-2e13-4d97-a1f0-ae2db947f07a",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469a191f-dc31-4e08-a8ca-10626c46876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following charachters will be striped: !\"#$%&'()*+,-./:;<=>?@\\^_`{|}~¿¡ \n",
      "\n",
      "Here is the reslut of standardization of the first few Spanish sentences: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Standardized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Puedes caminar?</td>\n",
       "      <td>b'puedes caminar'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom es mi padre.</td>\n",
       "      <td>b'tom es mi padre'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tenemos que acordarnos de recoger a Tom en la ...</td>\n",
       "      <td>b'tenemos que acordarnos de recoger a tom en l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No quiero oír más excusas.</td>\n",
       "      <td>b'no quiero o\\xc3\\xadr m\\xc3\\xa1s excusas'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le seguí a ella hasta la habitación.</td>\n",
       "      <td>b'le segu\\xc3\\xad a ella hasta la habitaci\\xc3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Original  \\\n",
       "0                                   ¿Puedes caminar?   \n",
       "1                                   Tom es mi padre.   \n",
       "2  Tenemos que acordarnos de recoger a Tom en la ...   \n",
       "3                         No quiero oír más excusas.   \n",
       "4               Le seguí a ella hasta la habitación.   \n",
       "\n",
       "                                        Standardized  \n",
       "0                                  b'puedes caminar'  \n",
       "1                                 b'tom es mi padre'  \n",
       "2  b'tenemos que acordarnos de recoger a tom en l...  \n",
       "3         b'no quiero o\\xc3\\xadr m\\xc3\\xa1s excusas'  \n",
       "4  b'le segu\\xc3\\xad a ella hasta la habitaci\\xc3...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a string of all the charachters to strip from the dataset.\n",
    "# Make sure to exclude the square brackets \"[\" and \"]\"  as we will be\n",
    "# using them in markers to mark the beginning and end of sentences in \n",
    "# the target language (English).\n",
    "\n",
    "strip_chars = string.punctuation+'¿¡'\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "print(f'The following charachters will be striped: {strip_chars} \\n')\n",
    "\n",
    "# Escape the special charachters via the regular expression package.\n",
    "# The square brackets indicate that there are multiple patterns.\n",
    "strip_chars = '['+re.escape(strip_chars)+']'\n",
    "\n",
    "# Define a standardization function for the text dataset.\n",
    "def custom_standardization(input_str):\n",
    "    '''Standardizes an input string or tensor of strings by\n",
    "    convering all the charachters to lower case and striping\n",
    "    the charachters in strip_chars. Returns a string valued\n",
    "    tf.tensor.\n",
    "    '''\n",
    "    return tf.strings.regex_replace(tf.strings.lower(input_str), strip_chars, \"\")\n",
    "\n",
    "print('Here is the reslut of standardization of the first few Spanish sentences: \\n')\n",
    "pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (df.loc[20:24,['es']], custom_standardization(df.loc[20:24,['es']]).numpy()),\n",
    "        axis=1), columns = ['Original', 'Standardized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223dbde8-f8ff-488b-a9de-b87b58db0ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "      <th>en_s</th>\n",
       "      <th>es_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We need to chat soon.</td>\n",
       "      <td>Necesitamos charlar pronto.</td>\n",
       "      <td>b'[start] we need to chat soon [end]'</td>\n",
       "      <td>b'necesitamos charlar pronto'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She married a musician.</td>\n",
       "      <td>Se casó con un músico.</td>\n",
       "      <td>b'[start] she married a musician [end]'</td>\n",
       "      <td>b'se cas\\xc3\\xb3 con un m\\xc3\\xbasico'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do it right now.</td>\n",
       "      <td>Hazlo ahoritita.</td>\n",
       "      <td>b'[start] do it right now [end]'</td>\n",
       "      <td>b'hazlo ahoritita'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the ticket line.</td>\n",
       "      <td>Esta es la cola para sacar los billetes.</td>\n",
       "      <td>b'[start] this is the ticket line [end]'</td>\n",
       "      <td>b'esta es la cola para sacar los billetes'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         en                                        es  \\\n",
       "0     We need to chat soon.               Necesitamos charlar pronto.   \n",
       "1   She married a musician.                    Se casó con un músico.   \n",
       "2          Do it right now.                          Hazlo ahoritita.   \n",
       "3  This is the ticket line.  Esta es la cola para sacar los billetes.   \n",
       "\n",
       "                                       en_s  \\\n",
       "0     b'[start] we need to chat soon [end]'   \n",
       "1   b'[start] she married a musician [end]'   \n",
       "2          b'[start] do it right now [end]'   \n",
       "3  b'[start] this is the ticket line [end]'   \n",
       "\n",
       "                                         es_s  \n",
       "0               b'necesitamos charlar pronto'  \n",
       "1      b'se cas\\xc3\\xb3 con un m\\xc3\\xbasico'  \n",
       "2                          b'hazlo ahoritita'  \n",
       "3  b'esta es la cola para sacar los billetes'  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add markers at the beginning and the end of every sentence in the target dataset.\n",
    "def en_sentence_marker(row: pd.Series) -> pd.Series:\n",
    "    '''Adds start and end markers to every string in a string valued\n",
    "    pandas.Series object. For English sentences, the start marker will\n",
    "    be '[start]' and the end marker '[end]'.\n",
    "    '''\n",
    "    return '[start] '+row+' [end]'\n",
    "\n",
    "# Create new columns for standardized English and Spanish sentences.\n",
    "# Make sure to mark the English (target) sentences.\n",
    "df['en_s'] = custom_standardization(df.en.apply(en_sentence_marker)).numpy()\n",
    "df['es_s'] = custom_standardization(df.es).numpy()\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f6166a-675b-4d55-a85c-74e1e2b2e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of enteries: 118964\n",
      "Numper of training samples: 95170\n",
      "Numper of validation samples: 11897\n",
      "Numper of test samples: 11897\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation and test datasets.\n",
    "valid_df = df.loc[np.floor(df.shape[0]*0.8)-1 : np.floor(df.shape[0]*0.9)-1]\n",
    "test_df = df.loc[np.floor(df.shape[0]*0.9) :]\n",
    "df = df.loc[:np.floor(df.shape[0]*0.8)-2]\n",
    "\n",
    "print(f'Total number of enteries: {df.shape[0]+valid_df.shape[0]+test_df.shape[0]}')\n",
    "print(f'Numper of training samples: {df.shape[0]}')\n",
    "print(f'Numper of validation samples: {valid_df.shape[0]}')\n",
    "print(f'Numper of test samples: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6d3c5-22a3-49a4-8265-a4b852fd4536",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Our NMT model will be tailored to translate relatively short sentences given our training dataset. As such we will set a maximum length for input and output sentences which will be imposed by the vectorization layers.\n",
    "\n",
    "Thus, we will analyze the training dataset to find suitable parameters for the vectorization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9499ab40-49d6-4b49-91c9-54311495aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique English tokens in the training dataset =  12622\n",
      "\n",
      "Number of unique Spanish tokens in the training dataset =  23860\n",
      "\n",
      "Sentence length quantiles for each language in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       en    es\n",
       "0.5   8.0   6.0\n",
       "0.9  12.0  10.0\n",
       "1.0  37.0  42.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of unique tokens in each language in the training dataset\n",
    "print('\\nNumber of unique English tokens in the training dataset = ',\n",
    "      tf.unique(tf.strings.split(df.en_s, ' ').flat_values)[0].__len__())\n",
    "\n",
    "print('\\nNumber of unique Spanish tokens in the training dataset = ',\n",
    "      tf.unique(tf.strings.split(df.es_s, ' ').flat_values)[0].__len__())\n",
    "\n",
    "# Find a reasonable cutoff length for sentences\n",
    "print('\\nSentence length quantiles for each language in the dataset:')\n",
    "pd.DataFrame(\n",
    "    tf.strings.split(df[['en_s','es_s']], ' ').row_lengths(axis=2).numpy()\n",
    ").quantile([0.5,0.9, 1.0]).rename(columns={0:'en', 1:'es'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc479c3-4578-4f1f-a7be-6835321650a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the vocabulary as well as the cutoff length of sentences.\n",
    "# Recall that our English sentences have markers and we do not care if the\n",
    "# [end] marker is dropped because of the cutoff.\n",
    "en_vocab_size = 12000\n",
    "en_seq_length = 16\n",
    "\n",
    "es_vocab_size = 20000\n",
    "es_seq_length = 16\n",
    "\n",
    "# Create a vectorization layer for the English dataset.\n",
    "en_vectorization = layers.TextVectorization(\n",
    "    max_tokens=en_vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=en_seq_length,\n",
    "    standardize=None,\n",
    "    name=f\"en_vectorization_{en_vocab_size}_{en_seq_length}\")\n",
    "\n",
    "# Create a vectorization layer for the Spanish dataset.\n",
    "es_vectorization = layers.TextVectorization(\n",
    "    max_tokens=es_vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=es_seq_length,\n",
    "    standardize=None,\n",
    "    name=f\"es_vectorization_{es_vocab_size}_{es_seq_length}\")\n",
    "\n",
    "# Load or adapt the vectorization layers vocabularies.\n",
    "if os.path.exists(home_dir+'/datasets/spa-eng/en_vocab.csv'):\n",
    "    en_vectorization.set_vocabulary(list(pd.read_csv(\n",
    "        home_dir+'/datasets/spa-eng/en_vocab.csv',\n",
    "        header=None).fillna('')[0]))\n",
    "else:\n",
    "    en_vectorization.adapt(df.en_s)\n",
    "\n",
    "if os.path.exists(home_dir+'/datasets/spa-eng/es_vocab.csv'):\n",
    "    es_vectorization.set_vocabulary(list(pd.read_csv(\n",
    "        home_dir+'/datasets/spa-eng/es_vocab.csv',\n",
    "        header=None).fillna('')[0]))\n",
    "else:\n",
    "    es_vectorization.adapt(df.es_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4fdbad-17a7-4dc7-b797-fe32a719ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary to disk\n",
    "if not os.path.exists(home_dir+'/datasets/spa-eng/en_vocab.csv'):\n",
    "    pd.DataFrame(en_vectorization.get_vocabulary()).to_csv(\n",
    "        home_dir+'/datasets/spa-eng/en_vocab.csv',\n",
    "        header=False,\n",
    "        index=False)\n",
    "if not os.path.exists(home_dir+'/datasets/spa-eng/es_vocab.csv'):\n",
    "    pd.DataFrame(es_vectorization.get_vocabulary()).to_csv(\n",
    "        home_dir+'/datasets/spa-eng/es_vocab.csv',\n",
    "        header=False,\n",
    "        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b309db-3514-4645-b714-9150a2172b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectors can be reverted back into text via a dictionary that maps\n",
    "# the indices to the vocabulary.\n",
    "en_vocabulary = en_vectorization.get_vocabulary()\n",
    "en_vocabulary = dict(zip(range(len(en_vocabulary)), en_vocabulary))\n",
    "\n",
    "es_vocabulary = es_vectorization.get_vocabulary()\n",
    "es_vocabulary = dict(zip(range(len(es_vocabulary)), es_vocabulary))\n",
    "\n",
    "def decode_en_vector(en_vector):\n",
    "    '''Transforms a vectorized English sentence back to\n",
    "    English text using the recorded vocabulary.\n",
    "    Returns a string.\n",
    "    '''\n",
    "    en_str = ''\n",
    "    for i in range(en_seq_length):\n",
    "        en_str += en_vocabulary[en_vector[i]]+' '\n",
    "        if en_vector[i] == 0:\n",
    "            break\n",
    "    return en_str.rstrip()\n",
    "\n",
    "def decode_es_vector(es_vector):\n",
    "    '''Transforms a vectorized Spanish sentence back to\n",
    "    Spanish text using the recorded vocabulary.\n",
    "    Returns a string.\n",
    "    '''\n",
    "    es_str = ''\n",
    "    for i in range(es_seq_length):\n",
    "        es_str += es_vocabulary[es_vector[i]]+' '\n",
    "        if es_vector[i] == 0:\n",
    "            break\n",
    "    return es_str.rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892c73d-bb54-48bb-8257-d269072f303f",
   "metadata": {},
   "source": [
    "### Sequence to sequence dataset\n",
    "\n",
    "Our Seq2seq models generate the output sentence one token at a time. Therefore, we need to prepare our (sample, label) datasets to match each sample = (source, target[:-1]) with a sequence of labels = [target[k] for k in range(1,n)] where n is the length of the tokenized target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "256ab1f9-fa04-4c51-b454-09ed7a58698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Set parallelization parameter\n",
    "num_cores = 12\n",
    "\n",
    "# Define a function to map the sentences (source, target) to \n",
    "# ((source, target[:-1]), [vectorize(target[k]) for k in range(1,n)])\n",
    "def tokenwise_seqs(source, target):\n",
    "    ''' Maps a pair of sentences (strings) to a tuple of\n",
    "    ((source, target[:-1]), [vectorize(target[k]) for k in range(1,n)]).\n",
    "    Use with dataset.map().\n",
    "    '''\n",
    "    partial_target = tf.strings.regex_replace(target, ' \\[end\\]', '')\n",
    "    next_target = tf.strings.regex_replace(target, '\\[start\\] ', '')\n",
    "    next_target = en_vectorization(next_target)\n",
    "    return ( (source, partial_target), next_target )\n",
    "\n",
    "# Define a function to prepare our dataset\n",
    "def tokenwise_dataset(source, target):\n",
    "    ''' Creates a tf.data.Dataset object from pairs of source, target\n",
    "    strings. The dataset elements are mapped to take the following form:\n",
    "    ((source, target[:-1]), [vectorize(target[k]) for k in range(1,n)])\n",
    "    '''\n",
    "    ds = tf.data.Dataset.from_tensor_slices((source, target)).batch(batch_size)\n",
    "    ds = ds.map(tokenwise_seqs, num_parallel_calls=num_cores)\n",
    "    return ds.shuffle(512).prefetch(128).cache()\n",
    "\n",
    "# Prepare the training, validation and test datasets\n",
    "ds_train = tokenwise_dataset(df.es_s, df.en_s)\n",
    "ds_valid = tokenwise_dataset(valid_df.es_s, valid_df.en_s)\n",
    "ds_test = tokenwise_dataset(test_df.es_s, test_df.en_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92766a6-3804-4c39-94b3-89278a44e226",
   "metadata": {},
   "source": [
    "## Encoder-Decoder model\n",
    "\n",
    "To build our NMT models, we start by implementing the skeleton of the encoder-decoder architecture as a keras.Model. The model glues together an encoder and a decoder such that the output of the encoder, a vector or a sequence of vectors in the latent space for each input sentence, is passed on to the decoder as an input (initial state). We also implement an inference function so as to produce the full predicted output sequence.\n",
    "\n",
    "To illustrate how inference should be implemented, consider a single call of the model with inputs=[source, target[:k]] where source is the source sentence and target[:k] is the first k tokens of the target sentence. With the above input, the model will predict the next token in the target sequence. Thus, the full inference (model.translate(source)) is a loop starting with inputs=[source, \"[start]\"] which generates the next target tokens target[1:] until either the \"[end]\" marker is generated or the maximum target sentence length is reached:\n",
    "\n",
    "    [source, \"[start]\"] -> model -> t[1] -> [source, \"[start] \"+t[1]] -> model -> ... -> t[n] = \"[end]\" -> \"[start] ... [end]\".\n",
    "\n",
    "Note, however, that if we use this method during training -- especially in the initial phase of the training -- almost every predicted token will be incorrect. Since output[k] depends on output[:k], if output[:k] is mostly wrong the training step will have its effect significantly diminished. To overcome this issue, we can have the model feed the True values (target[:k]) to the decoder instead of output[:k]. This is referred to as **teacher forcing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f0982d0-8a53-4c14-ad1e-073076ca941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Encoder-Decoder architecture\n",
    "def Encoder_Decoder_NMT(source_embedding: layers.Layer,\n",
    "                        target_embedding: layers.Layer,\n",
    "                        encoder: layers.Layer,\n",
    "                        decoder: layers.Layer, \n",
    "                        name=\"encoder_decoder_NMT\"):\n",
    "    '''\n",
    "    '''\n",
    "    # Inputs\n",
    "    source = keras.Input(shape=(), dtype=tf.string, name=\"source_input\")\n",
    "    target = keras.Input(shape=(), dtype=tf.string, name=\"target_input\")\n",
    "    # Embeddings\n",
    "    x = source_embedding(source)\n",
    "    y = target_embedding(target)\n",
    "    # Encoding\n",
    "    x = encoder(x)\n",
    "    # Decoding\n",
    "    y = decoder([x, y])\n",
    "    #\n",
    "    return keras.Model(inputs = [source, target], outputs = y)\n",
    "\n",
    "# Implementation of the string to string inference\n",
    "def translate(source, NMT_model, vocab_dict, max_seq_len=16, target=\"[start]\"):\n",
    "    ''' The inference \"string to string\" function for NMT_model. \n",
    "    It uses greedy search (best next token) to translate the \n",
    "    source string (sentence in the source language) to the target\n",
    "    string (sentence in the target language).\n",
    "    \n",
    "    Inputs:\n",
    "        source: A string, sentence in the source language.\n",
    "\n",
    "        NMT_model: An encoder-decoder NMT keras.Model.\n",
    "\n",
    "        vocab_dict: A dictionary of the form {index:token} matching\n",
    "        the vocabulary of the target(decoder) vectorization layer.\n",
    "\n",
    "        max_seq_len: An integer specifying the maximum length of\n",
    "        the output (max number of tokens).\n",
    "\n",
    "        target: A string matching the start marker used in the target\n",
    "        sentences.\n",
    "    '''\n",
    "    source = tf.constant([source], dtype=tf.string)\n",
    "    target = tf.constant([target], dtype=tf.string)\n",
    "    \n",
    "    for i in range(max_seq_len):\n",
    "        # Infer next target token\n",
    "        next_target = vocab_dict[np.argmax(\n",
    "            NMT_model.predict([source, target], verbose=0)[0,i,:])]\n",
    "        # Add the inferred token to the target string\n",
    "        target = tf.strings.join([target, next_target], \" \")\n",
    "        # If \"[end]\" token is generated as the next target, break the loop.\n",
    "        if next_target == \"[end]\":\n",
    "            break\n",
    "    return target.numpy()[0].decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b33a2-bdca-4fbf-8021-12e33825f1af",
   "metadata": {},
   "source": [
    "### Positional embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb5e1f1c-220d-4c8e-b2f9-54b7da83c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional embedding\n",
    "class Custom_Embedding(layers.Layer):\n",
    "    ''' An embedding layer that generates text embeddings based on\n",
    "    both the token and its position. It does so by combining the\n",
    "    token embedding layer with an additional position embedding layer\n",
    "    position_embedding: [0,1,2,...] -> v with v.shape = (seq_len,embed_dim).\n",
    "\n",
    "    Arguments:\n",
    "        max_seq_len: An integer limiting the maximum length of output sequence.\n",
    "\n",
    "        vocab_size: An integer corresponding to the size of the vectorization\n",
    "        vocabulary.\n",
    "\n",
    "        embed_dim: An integer setting the dimension of the embedding space.\n",
    "\n",
    "        vectorization: None or a pre-adapted TextVectorization layer.\n",
    "        If None, a new vectorization layer is instantiated.\n",
    "\n",
    "        token_embedding: None or a pre-defined token embedding layer.\n",
    "        If None, a new embedding layer is instantiated.\n",
    "        If an embedding layer with input dtype=string is provided,\n",
    "        the embedding layer is used as a combined vectorization+embedding\n",
    "        and no separate vectorization layer is used.\n",
    "\n",
    "        positional: bool, whether to use positional embedding or not. If\n",
    "        True, a new embedding layer is instantiated with\n",
    "        vocab_size = max_seq_len, embed_dim = embed_dim\n",
    "        The positional embedding is the sum of the token embedding and\n",
    "        the positional embedding:\n",
    "        embedding('hello') = token_embedding(hello) + position_embedding(0)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 max_seq_len,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 vectorization = None,\n",
    "                 token_embedding = None,\n",
    "                 positional = True,\n",
    "                 name = \"custom_embedding\",\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Arguments\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional = positional\n",
    "        # Check if vectorization layer is needed\n",
    "        vectorize = token_embedding is None or (\n",
    "            token_embedding.get_config()['dtype'] is not tf.string)\n",
    "        # Vectorization\n",
    "        if not vectorize:\n",
    "            self.vectorization = None\n",
    "        elif vectorization is not None:\n",
    "            # Use provided vectorization\n",
    "            assert type(vectorization) == layers.TextVectorization\n",
    "            assert vectorization.vocabulary_size() == vocab_size\n",
    "            self.vectorization = vectorization\n",
    "        else:\n",
    "            # Instantiate a new vectorization layer\n",
    "            self.vectorization = layers.TextVectorization(\n",
    "                max_tokens=vocab_size,\n",
    "                output_mode=\"int\",\n",
    "                output_sequence_length=max_seq_len,\n",
    "                standardize=None,\n",
    "                name=name+f\"_vec_{vocab_size}_{max_seq_len}\")\n",
    "        # Token embedding\n",
    "        if token_embedding is not None:\n",
    "            # Use provided embedding\n",
    "            assert token_embedding.variables[0].shape == (vocab_size, embed_dim)\n",
    "            self.token_embedding = token_embedding\n",
    "        else:\n",
    "            # Instantiate a new embedding layer for token embedding\n",
    "            self.token_embedding = layers.Embedding(\n",
    "                vocab_size, embed_dim, mask_zero=True)\n",
    "        # Sequence index embedding\n",
    "        if not positional:\n",
    "            self.position_embedding = None\n",
    "        else:\n",
    "            # Instantiate a new embedding layer for sequence index embedding\n",
    "            self.position_embedding = layers.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = inputs\n",
    "        if self.vectorization is not None:\n",
    "            y = self.vectorization(y)\n",
    "        y = self.token_embedding(y)\n",
    "        if self.positional:\n",
    "            seq_index = tf.range(tf.shape(y)[-2])\n",
    "            p = self.position_embedding(seq_index)\n",
    "            y = tf.add(y, p)\n",
    "        return y\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        if self.vectorization is not None:\n",
    "            x = self.vectorization(x)\n",
    "        return self.token_embedding.compute_mask(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"dtype\" : tf.string,\n",
    "                       \"max_seq_len\" : self.max_seq_len,\n",
    "                       \"vocab_size\" : self.vocab_size,\n",
    "                       \"embed_dim\" : self.embed_dim})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa1057-cf03-47a7-a57c-4008fe83a10f",
   "metadata": {},
   "source": [
    "## Recurrent NMT model\n",
    "\n",
    "Lets build as our first example a simple RNN model with only one recurrent layer in the encoder/decoder. We will use uni-directional Gated Recurrent Unit layers below. You will get a similar performance using LSTM or bidirectional recurrent layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b81a4-5a63-49ba-b820-0b436732bbcc",
   "metadata": {},
   "source": [
    "### RNN Encoder\n",
    "\n",
    "We will use a single GRU layer as our encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e95d1b3b-cf13-4375-871c-4b2b9e260ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple gru encoder class to serve as our encoder block\n",
    "class simple_gru_encoder(layers.Layer):\n",
    "    ''' Initiates a simple encoder block with a single \n",
    "    (uni-directional) Gated Recurrent Unit (GRU) layer.\n",
    "\n",
    "    Args:\n",
    "    embed_dim: An integer specifying the dimension of the \n",
    "    embedding space. input_shape = (None, None, embed_dim)\n",
    "    \n",
    "    latend_dim: An integer specifying the dimension of the\n",
    "    latent space. output_shape=(None, latent_dim)\n",
    "    '''\n",
    "    def __init__(self, embed_dim, latent_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        # Enable mask propagation\n",
    "        self.supports_masking = True\n",
    "        # Instantiate GRU layer\n",
    "        self.gru = layers.GRU(latent_dim,)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.gru(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"embed_dim\":self.embed_dim,\n",
    "                       \"latent_dim\":self.latent_dim,\n",
    "                       \"vocab_size\":self.vocab_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e44df8-0b64-4f5a-9001-3ac7602214f1",
   "metadata": {},
   "source": [
    "### RNN Decoder\n",
    "\n",
    "Our decoder block also consists of a single GRU layer with sequence outputs, as well as a (softmax activated) dense layer with units=target_vocab_size to make predictions. When the vocabulary size is too large, we can instead generate vectors in the embedding dimension. Also, for large enough latent_dim (>= 256) the model tends to overfit so we include a dropout layer to mitigate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c225e5-93ca-4afc-b53c-e733dee1dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for our simple decoder block\n",
    "class simple_gru_decoder(layers.Layer):\n",
    "    ''' Initiates a simple decoder block with a single\n",
    "    (uni-directional) GRU layer.\n",
    "\n",
    "    Args:\n",
    "    embed_dim: An integer specifying the dimension of the embedding \n",
    "    space for the target language.\n",
    "    latent_dim: An integer specifying the dimension of the latent\n",
    "    space. Determines the shape of input1 (batch_axis=0):\n",
    "        input0_shape = (None,), dtype = string,\n",
    "        input1_shape = (None, latent_dim), dtype = float.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, latent_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        # Enable mask propagation\n",
    "        self.supports_masking = True\n",
    "        # Instantiate GRU layer\n",
    "        self.gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "        # Target prediction layer\n",
    "        self.dense_predict = layers.Dense(output_dim, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        y = self.gru(y, initial_state=x)\n",
    "        y = layers.Dropout(0.5)(y)\n",
    "        y = self.dense_predict(y)\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"embed_dim\":self.embed_dim,\n",
    "                       \"latent_dim\":self.latent_dim,\n",
    "                       \"vocab_size\":self.vocab_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28835bf0-30bc-443f-986b-f3a28cf74b59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Masking behavior\n",
    "\n",
    "If you are curious about how masks are propagated you can unquote and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16c4f8d-ec81-4c54-bad2-0a449579b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking behavior\n",
    "\n",
    "'''\n",
    "# Create a custom embedding layer\n",
    "custom_en_embedding = Custom_Embedding(\n",
    "    max_seq_len = en_seq_length,\n",
    "    vocab_size = en_vocab_size,\n",
    "    embed_dim = 32,\n",
    "    vectorization = en_vectorization,\n",
    "    token_embedding = None,\n",
    "    positional = True)\n",
    "\n",
    "print('The mask generated for the string \"test\" is: \\n',\n",
    "      custom_en_embedding.compute_mask(\"test\").numpy())\n",
    "\n",
    "# Create an RNN encoder\n",
    "custom_en_encoder = simple_gru_encoder(32, 128)\n",
    "\n",
    "# With proper masking, the latent vector encoding of the\n",
    "# empty string \"\" should be [0,...,0].\n",
    "print('The latent vector encoding of the empty string \"\" satisfies:\\n MSE(v,[0,...]) =',\n",
    "      keras.losses.MSE(custom_en_encoder(custom_en_embedding([\"\"])), [[0.]*128]))\n",
    "\n",
    "embedded_vector = custom_en_embedding([\"\"]).numpy()\n",
    "print('Without proper masking we have:\\n MSE(v_unmasked,[0,...]) =',\n",
    "      keras.losses.MSE(custom_en_encoder(embedded_vector), [[0.]*128]))\n",
    "\n",
    "# Create an RNN decoder\n",
    "custom_en_decoder = simple_gru_decoder(32, 128, en_vocab_size)\n",
    "\n",
    "sample = tf.constant([''], dtype=tf.string)\n",
    "embedded_sample = custom_en_embedding(sample)\n",
    "encoded_sample = custom_en_encoder(embedded_sample)\n",
    "encoder_sample_mask = custom_en_encoder.compute_mask(embedded_sample)\n",
    "decoded_sample = custom_en_decoder([encoded_sample, embedded_sample])\n",
    "decoded_sample_unmasked = custom_en_decoder([encoded_sample.numpy(),\n",
    "                                             embedded_sample.numpy()])\n",
    "\n",
    "print('The encoded latent vector has mask =', encoder_sample_mask)\n",
    "print('With the inital biases set to zero, we should get a uniform output:\\n',\n",
    "      tf.reduce_all(decoded_sample == 1.0/en_vocab_size).numpy())\n",
    "print('Withough proper masking, the output no longer uniform:\\n',\n",
    "      not tf.reduce_all(decoded_sample_unmasked == 1.0/en_vocab_size).numpy())\n",
    "\n",
    "del custom_en_embedding\n",
    "del custom_en_encoder\n",
    "del custom_en_decoder\n",
    "del sample\n",
    "del encoded_sample\n",
    "del encoder_sample_mask\n",
    "del decoded_sample\n",
    "del decoded_sample_unmasked\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59602c6b-ffeb-4bf5-8d50-b7454c13d8a8",
   "metadata": {},
   "source": [
    "### RNN Spanish to English translation model\n",
    "\n",
    "We can now glue the two blocks together to build the full model to train before using for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0990e1ec-5d59-4069-a5e8-3b344fc16d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source_input (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " custom__embedding (Custom_Embe  (None, 16, 50)      1000000     ['source_input[0][0]']           \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " target_input (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " simple_gru_encoder (simple_gru  (None, 512)         866304      ['custom__embedding[0][0]']      \n",
      " _encoder)                                                                                        \n",
      "                                                                                                  \n",
      " custom__embedding_1 (Custom_Em  (None, 16, 50)      600000      ['target_input[0][0]']           \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " simple_gru_decoder (simple_gru  (None, 16, 12000)   7022304     ['simple_gru_encoder[0][0]',     \n",
      " _decoder)                                                        'custom__embedding_1[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,488,608\n",
      "Trainable params: 9,488,608\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Choose the embedding dimension for the source language (Spanish).\n",
    "es_embed_dim = 50\n",
    "# Choose the embedding dimension for the target language (English).\n",
    "en_embed_dim = 50\n",
    "# Choose the dimension of the latent space (Encoder.output_shape[-1]).\n",
    "latent_dim = 512\n",
    "# Choose a name for the model\n",
    "gruNMT_name = f'gruNMT_es2en_{es_embed_dim}_{en_embed_dim}_{latent_dim}'\n",
    "# Set training arguments\n",
    "NMT_optimizer = \"adam\"\n",
    "NMT_loss = \"sparse_categorical_crossentropy\"\n",
    "NMT_metrics = [\"accuracy\"]\n",
    "NMT_callbacks = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Instantiate a Encoder_Decoder_NMT model with our simple RNN based \n",
    "# encoder and decoder.\n",
    "gruNMT_model = Encoder_Decoder_NMT(\n",
    "    source_embedding = Custom_Embedding(\n",
    "        es_seq_length, es_vocab_size, es_embed_dim, es_vectorization, positional=False),\n",
    "    target_embedding = Custom_Embedding(\n",
    "        en_seq_length, en_vocab_size, en_embed_dim, en_vectorization, positional=False),\n",
    "    encoder = simple_gru_encoder(es_embed_dim, latent_dim),\n",
    "    decoder = simple_gru_decoder(en_embed_dim, latent_dim, output_dim=en_vocab_size),\n",
    "    name = gruNMT_name)\n",
    "\n",
    "# Build the model by calling a sample input\n",
    "gruNMT_model([df.es_s.iloc[:4], df.en_s.iloc[:4]])\n",
    "\n",
    "# Compile model\n",
    "gruNMT_model.compile(optimizer=NMT_optimizer, loss=NMT_loss, metrics=NMT_metrics)\n",
    "\n",
    "# Print out the model summary\n",
    "gruNMT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f3c397a-e6e3-4d66-b78e-411429b1ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model/load weights from file\n",
    "model_path = home_dir+f'/models/07_nmt/{gruNMT_name}/{gruNMT_name}'\n",
    "if not os.path.exists(model_path+\".tf.index\"):\n",
    "    gruNMT_history = gruNMT_model.fit(ds_train,\n",
    "                                      validation_data=ds_valid,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=60,\n",
    "                                      callbacks=NMT_callbacks)\n",
    "else:\n",
    "    gruNMT_model.load_weights(model_path+\".tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d3a01f0-b97a-43ad-ae2e-a934be3ca6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists!\n"
     ]
    }
   ],
   "source": [
    "# Save weights to file\n",
    "if not os.path.exists(home_dir+'/models/07_nmt'):\n",
    "    os.mkdir(home_dir+'/models/07_nmt')\n",
    "if not os.path.exists(home_dir+f'/models/07_nmt/{gruNMT_name}'):\n",
    "    os.mkdir(home_dir+f'/models/07_nmt/{gruNMT_name}')\n",
    "\n",
    "if not os.path.exists(model_path+\".tf.index\"):\n",
    "    gruNMT_model.save_weights(model_path+\".tf\")\n",
    "else:\n",
    "    print('File already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd7c2f-483b-46c2-ba03-3e0e6b34043d",
   "metadata": {},
   "source": [
    "### Evaluation and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ede4de40-f5fd-4e33-b70a-e88b20cc2c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 64s 1s/step - loss: 0.8621 - accuracy: 0.6663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8621478080749512, 0.6663393378257751]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model performance on the test dataset\n",
    "gruNMT_model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7b19c45-374a-402d-919c-92093716eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can come and see me whenever it's convenient for you. --->  [start] can you come when we see me again [end]\n",
      "Tom is drawing something. --->  [start] tom is chewing something [end]\n",
      "I was thinking about you a lot today. --->  [start] i was looking for you very much today [end]\n",
      "Air is mainly composed of nitrogen and oxygen. --->  [start] air is mainly composed of nitrogen and oxygen [end]\n"
     ]
    }
   ],
   "source": [
    "for row in zip(test_df.es_s.iloc[:4], test_df.en.iloc[:4]):\n",
    "    es_sample, en_sample = row\n",
    "    en_translation = translate(es_sample, gruNMT_model, en_vocabulary, en_seq_length)\n",
    "    print(en_sample+' ---> ', en_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866ea28-a525-4c20-8631-131549590158",
   "metadata": {},
   "source": [
    "## Transformer NMT model\n",
    "\n",
    "Next, we will build a neural machine translation model with the transformer architecture first proposed in the paper \"<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>\". The architecture is depicted below and is designed with strong emphasis on context which is the focus of the attention layers. In fact, were we to use standard embedding layers, the model would be blind to token order in the sequence. To remedy this, we essentially \"mark\" each token embedding with a mark generated from the position at which it appears in the sequence.\n",
    "\n",
    "In the original paper, the marks used were a fixed set of embedding vectors. We can just as well switch to a random set of embedding vectors or even promote them to trainable weights which is the precisely what we have done in the implementation of posistional embedding in the Custom_Embedding layer above.\n",
    "\n",
    "![Transformer Architecture](https://raw.githubusercontent.com/ndoroud/BA885/master/images/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb04d7d-7b95-4d9f-94cf-d615464a6ee6",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9be91f98-62e4-4790-9712-48c841e18ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of transformer encoder\n",
    "class transformer_encoder(layers.Layer):\n",
    "    ''' The basic Transformer Encoder with multi-head attention. The layer\n",
    "    takes sequences of vectors corresponding to (positionally) embedded \n",
    "    tokens and returns a tensor. With batch axis=0 we have:\n",
    "        input_shape = (None, None, embed_dim)\n",
    "        output_shape = (None, None, embed_dim).\n",
    "    (Assumes source_embed_dim == target_embed_dim)\n",
    "    \n",
    "    Arguments:\n",
    "        embed_dim: An integer specifying the dimension of the \n",
    "        embedding space used for positional embedding.\n",
    "        \n",
    "        proj_dim: An integer specifying the dimension of the\n",
    "        projection space.\n",
    "\n",
    "        num_heads: An integer setting the number of head for the\n",
    "        multi-head attention layer.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, proj_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Arguments\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dim = proj_dim\n",
    "        # Enable mask propagation\n",
    "        self.supports_masking = True\n",
    "        # Attention layer\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Dense projection layers\n",
    "        self.dense_downscale = layers.Dense(proj_dim, activation=\"relu\")\n",
    "        self.dense_upscale = layers.Dense(embed_dim, activation=None)\n",
    "        # Normalization layers\n",
    "        self.attention_norm = layers.LayerNormalization()\n",
    "        self.output_norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # Reshape mask for the attention layer\n",
    "        # (batch_size, seq_len) -> (batch_size, 1, seq_len)\n",
    "        if mask is not None:\n",
    "            mask = tf.expand_dims(mask, axis=1)\n",
    "        # Attention\n",
    "        y = self.attention(x, x, attention_mask = mask)\n",
    "        # Residual connection with inputs\n",
    "        y = tf.add(y, x)\n",
    "        y = self.attention_norm(y)\n",
    "        # Keep a copy for residual connection\n",
    "        x = y\n",
    "        # Apply projection\n",
    "        y = self.dense_downscale(y)\n",
    "        y = self.dense_upscale(y)\n",
    "        # Residual connection\n",
    "        y = tf.add(y, x)\n",
    "        y = self.output_norm(y)\n",
    "        mask = None\n",
    "        return y\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"embed_dim\" : self.embed_dim,\n",
    "                       \"projection_dim\" : self.proj_dim,\n",
    "                       \"num_attention_heads\" : self.num_heads})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5d42b83-a2fa-4736-b0b9-2b452decbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify masking behavior\n",
    "\n",
    "'''\n",
    "custom_en_embedding = Custom_Embedding(\n",
    "    max_seq_len = en_seq_length,\n",
    "    vocab_size = en_vocab_size,\n",
    "    embed_dim = 32,\n",
    "    vectorization = en_vectorization,\n",
    "    token_embedding = None,\n",
    "    positional = True)\n",
    "\n",
    "custom_en_encoder = transformer_encoder(32, 128, 4)\n",
    "\n",
    "print('The encoded output has mask =',\n",
    "      custom_en_encoder.compute_mask(custom_en_embedding([\"\"])))\n",
    "\n",
    "print('With proper masking we should get uniform attention scores for the empty string:\\n',\n",
    "      tf.reduce_all(custom_en_encoder.attention(\n",
    "          custom_en_embedding([\"\"]), custom_en_embedding([\"\"]),\n",
    "          attention_mask = tf.expand_dims(custom_en_embedding.compute_mask(['']), axis=1),\n",
    "          return_attention_scores = True)[1] == 1/en_seq_length).numpy())\n",
    "\n",
    "print('Without masking are the scores still uniform?\\n',\n",
    "      tf.reduce_all(custom_en_encoder.attention(\n",
    "          custom_en_embedding([\"\"]).numpy(), custom_en_embedding([\"\"]).numpy(),\n",
    "          attention_mask = None,\n",
    "          return_attention_scores = True)[1] == 1/en_seq_length).numpy())\n",
    "\n",
    "del custom_en_embedding\n",
    "del custom_en_encoder\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051c1a7-e666-4350-956b-631371261da8",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62835f8c-921e-4466-aa9a-180cba27f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Encoder-Decoder architecture\n",
    "class transformer_decoder(layers.Layer):\n",
    "    ''' The basic Transformer Decoder and multi-head attention.\n",
    "    The layer takes pairs of tensor inputs representing the encoded\n",
    "    source and the embedded target. The decoder outputs are sequences\n",
    "    of softmax vectors over the target vocabulary index.\n",
    "    With batch_axis=0, we have:\n",
    "        input_shapes = [(None, None, embed_dim), (None, None, embed_dim)]\n",
    "        output_shape = (None, None, output_dim=target_vocab_size).    \n",
    "\n",
    "    Arguments:\n",
    "        embed_dim: An integer specifying the dimension of the \n",
    "        embedding space used for positional embedding.\n",
    "\n",
    "        proj_dim: An integer specifying the dimension of the\n",
    "        projection space.\n",
    "        \n",
    "        num_heads: An integer setting the number of head for the\n",
    "        multi-head attention layer.\n",
    "        \n",
    "        output_dim: An integer setting the output shape of the layer.\n",
    "        output_dim should equal target vocabulary size.\n",
    "    '''\n",
    "    def __init__(self, embed_dim, proj_dim, num_heads, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Arguments\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj_dim = proj_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.output_dim = output_dim\n",
    "        # Enable mask propagation\n",
    "        self.supports_masking = True\n",
    "        # Attention layers\n",
    "        self.self_attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.encoder_attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Dense projection layers\n",
    "        self.dense_downscale = layers.Dense(proj_dim, activation=\"relu\")\n",
    "        self.dense_upscale = layers.Dense(embed_dim, activation=None)\n",
    "        # Normalization layers\n",
    "        self.self_attention_norm = layers.LayerNormalization()\n",
    "        self.encoder_attention_norm = layers.LayerNormalization()\n",
    "        self.projection_norm = layers.LayerNormalization()\n",
    "        # Target prediction layer\n",
    "        self.dense_predict = layers.Dense(output_dim, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, mask=[None, None]):\n",
    "        # inputs = encoder(embedded_source), embedded_target\n",
    "        x, y = inputs\n",
    "        batch_size = tf.shape(y)[0]\n",
    "        seq_len = tf.shape(y)[1]\n",
    "        # Create sequence/causal mask (vector -> sequence)\n",
    "        seq_mask = tf.expand_dims(\n",
    "            tf.cast(\n",
    "                tf.expand_dims(tf.range(seq_len), axis=-1) >= tf.range(seq_len),\n",
    "                dtype=tf.int32),\n",
    "            axis = 0)\n",
    "        # seq_mask.shape = (1, seq_len, seq_len)\n",
    "        # tile with multiplicity = [batch_size, 1, 1] \n",
    "        seq_mask = tf.tile(\n",
    "            seq_mask,\n",
    "            tf.pad(tf.expand_dims(batch_size, -1),\n",
    "                   tf.constant([[0,2]]),\n",
    "                   constant_values=1))\n",
    "        # seq_mask.shape = (batch_size, seq_len, seq_len)\n",
    "        # Reshape the target padding mask for attention layer and cast as int.\n",
    "        encoder_mask, target_mask = mask\n",
    "        if target_mask is not None:\n",
    "            target_mask = tf.cast(tf.expand_dims(target_mask, axis=1), dtype=tf.int32)\n",
    "            # target_mask.shape = (batch_size, 1, seq_len)\n",
    "            target_mask = tf.minimum(target_mask, seq_mask)\n",
    "        # target_mask.shape = seq_mask.shape = (batch_size, seq_len, seq_len)\n",
    "        # Keep a copy of embedded_target for residual connection\n",
    "        r = y\n",
    "        # Target self-attention (query=value=key=target)\n",
    "        y = self.self_attention(r, r, attention_mask = seq_mask)\n",
    "        # Residual connection (add & normalize)\n",
    "        y = tf.add(y, r)\n",
    "        y = self.self_attention_norm(y)\n",
    "        # Keep a copy for residual connection\n",
    "        r = y\n",
    "        # Target-Encoder attention\n",
    "        # (query=partially decoded target, value=key=encoded source)\n",
    "        y = self.encoder_attention(r, x, x, attention_mask = target_mask)\n",
    "        # Residual connection (add & normalize)\n",
    "        y = tf.add(y, r)\n",
    "        y = self.encoder_attention_norm(y)\n",
    "        # Keep a copy for residual connection\n",
    "        r = y\n",
    "        # Apply projection\n",
    "        y = self.dense_downscale(r)\n",
    "        y = self.dense_upscale(y)\n",
    "        # Residual connection (add & normalize)\n",
    "        y = tf.add(y, r)\n",
    "        y = self.projection_norm(y)\n",
    "        # Pass on to the prediction layer\n",
    "        y = layers.Dropout(0.5)(y)\n",
    "        y = self.dense_predict(y)\n",
    "        return y\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"embed_dim\" : self.embed_dim,\n",
    "                       \"num_attention_heads\" : self.num_heads,\n",
    "                       \"projection_dim\" : self.proj_dim, \n",
    "                       \"output_dim\" : self.output_dim})\n",
    "        return config\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19683317-c76b-485a-b706-dec312defd0f",
   "metadata": {},
   "source": [
    "### Transformer Spanish to English translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1300180e-dbca-4dc9-8646-de6af04db4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source_input (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " custom__embedding_4 (Custom_Em  (None, 16, 160)     3202560     ['source_input[0][0]']           \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " target_input (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_1 (transfo  (None, 16, 160)     865088      ['custom__embedding_4[0][0]']    \n",
      " rmer_encoder)                                                                                    \n",
      "                                                                                                  \n",
      " custom__embedding_5 (Custom_Em  (None, 16, 160)     1922560     ['target_input[0][0]']           \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " transformer_decoder_1 (transfo  (None, 16, 12000)   3620608     ['transformer_encoder_1[0][0]',  \n",
      " rmer_decoder)                                                    'custom__embedding_5[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,610,816\n",
      "Trainable params: 9,610,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Choose the embedding dimension for the source language (Spanish).\n",
    "es_embed_dim = 160\n",
    "# Choose the embedding dimension for the target language (English).\n",
    "en_embed_dim = 160\n",
    "# Choose the projection dimension\n",
    "proj_dim = 128\n",
    "# Choose the number of heads on multi-head attention layers\n",
    "num_heads = 8\n",
    "# Choose a name for the model\n",
    "transformerNMT_name = 'transformerNMT_es2en_{}_{}_{}_{}'.format(\n",
    "    es_embed_dim, en_embed_dim, proj_dim, num_heads)\n",
    "# Set training arguments\n",
    "NMT_optimizer = \"adam\"\n",
    "NMT_loss = \"sparse_categorical_crossentropy\"\n",
    "NMT_metrics = [\"accuracy\"]\n",
    "NMT_callbacks = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Instantiate a Encoder_Decoder_NMT model with our transformer\n",
    "# encoder and decoder.\n",
    "transformerNMT_model = Encoder_Decoder_NMT(\n",
    "    source_embedding = Custom_Embedding(\n",
    "        es_seq_length, es_vocab_size, es_embed_dim, es_vectorization, positional=True),\n",
    "    target_embedding = Custom_Embedding(\n",
    "        en_seq_length, en_vocab_size, en_embed_dim, en_vectorization, positional=True),\n",
    "    encoder = transformer_encoder(es_embed_dim, proj_dim, num_heads),\n",
    "    decoder = transformer_decoder(en_embed_dim, proj_dim, num_heads, en_vocab_size),\n",
    "    name = transformerNMT_name)\n",
    "\n",
    "# Build the model by calling a sample input\n",
    "transformerNMT_model([df.es_s.iloc[:4], df.en_s.iloc[:4]])\n",
    "\n",
    "# Compile model\n",
    "transformerNMT_model.compile(\n",
    "    optimizer=NMT_optimizer,\n",
    "    loss=NMT_loss,\n",
    "    metrics=NMT_metrics)\n",
    "\n",
    "# Print out the model summary\n",
    "transformerNMT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4119a6a-cd8a-4fcc-9e14-6f863688c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model/load weights from file\n",
    "model_path = home_dir+f'/models/07_nmt/{transformerNMT_name}/{transformerNMT_name}'\n",
    "if not os.path.exists(model_path+\".tf.index\"):\n",
    "    transformerNMT_history = transformerNMT_model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_valid,\n",
    "        batch_size=batch_size,\n",
    "        epochs=40,\n",
    "        callbacks=NMT_callbacks)\n",
    "else:\n",
    "    transformerNMT_model.load_weights(model_path+\".tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ec3590b-df44-4d98-af07-0f74f2761a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists!\n"
     ]
    }
   ],
   "source": [
    "# Save weights to file\n",
    "if not os.path.exists(home_dir+'/models/07_nmt'):\n",
    "    os.mkdir(home_dir+'/models/07_nmt')\n",
    "if not os.path.exists(home_dir+f'/models/07_nmt/{transformerNMT_name}'):\n",
    "    os.mkdir(home_dir+f'/models/07_nmt/{transformerNMT_name}')\n",
    "\n",
    "if not os.path.exists(model_path+\".tf.index\"):\n",
    "    transformerNMT_model.save_weights(model_path+\".tf\")\n",
    "else:\n",
    "    print('File already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50cfb622-545b-40e7-b25d-dc0f53118a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 51s 1s/step - loss: 0.7184 - accuracy: 0.6924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7183948755264282, 0.6923847794532776]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model performance on the test dataset\n",
    "transformerNMT_model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f05dbec-fa45-4043-9fa4-e8ce8ee56836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can come and see me whenever it's convenient for you. --->  [start] can you come and see me when you come well [end]\n",
      "Tom is drawing something. --->  [start] tom is something wrong [end]\n",
      "I was thinking about you a lot today. --->  [start] i was thinking about you having a lot of you much party [end]\n",
      "Air is mainly composed of nitrogen and oxygen. --->  [start] the air mainly consists of nitrogen and oxygen [end]\n"
     ]
    }
   ],
   "source": [
    "for row in zip(test_df.es_s.iloc[:4], test_df.en.iloc[:4]):\n",
    "    es_sample, en_sample = row\n",
    "    en_translation = translate(es_sample, transformerNMT_model, en_vocabulary, en_seq_length)\n",
    "    print(en_sample+' ---> ', en_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897d556-d04d-471d-9e4b-c0066d6715da",
   "metadata": {},
   "source": [
    "## Using a pre-trained embedding\n",
    "\n",
    "You can try using a pre-trained embedding to help boost your model performance. The cells below download and prepare the Google NNLM Spanish embedding with normalization and embedding dimension of 50. The pre-trained embedding is loaded as a keras layer and can be passed to our Custom_Embedding layer to promote it to a positional embedding.\n",
    "\n",
    "Note, the pre-trained embedding generates sequences of variable length, as such you need to modify the dataset to pad/truncate the embeddings to the desired sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "267a028d-b621-4a06-bf6f-9953b1acc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to download a google NNLM model\n",
    "def download_g_nnlm(nnlm_model: str):\n",
    "    '''Downloads the NNLM model from Google's NNLM models on kaggle\n",
    "    (https://www.kaggle.com/models/google/nnlm) where pretrained\n",
    "    embedding models in multiple languages are available. You can\n",
    "    choose the embedding dimension to be 50 or 128. Finally, you can\n",
    "    choose models with or without text normalization to remove \n",
    "    punctuations.\n",
    "\n",
    "    The downloaded model is moved to:\n",
    "        home_dir/models/google_nnlm/lled(wn)\n",
    "        ll: 2 letter abbreviation of the language (en, es, de,...)\n",
    "        ed: embedding dimension (50 or 128)\n",
    "        wn: with normalization\n",
    "    '''\n",
    "    # model specs\n",
    "    model = nnlm_model.split('/')[-1]\n",
    "    language = model.split('-')[0]\n",
    "    embedding_dim = model.split('-')[1].split('dim')[-1]\n",
    "    model_dir = f'{home_dir}/models/google_nnlm/{language}{embedding_dim}'\n",
    "    if len(model.split('-'))>2:\n",
    "        model_dir += '_wn'\n",
    "    # Check if the model is on disk\n",
    "    if os.path.exists(model_dir):\n",
    "        print('Model is already on disk.')\n",
    "        return None\n",
    "    # download model\n",
    "    path = kagglehub.model_download(nnlm_model, force_download=True)\n",
    "    # Create model_dir\n",
    "    os.mkdir(model_dir)\n",
    "    # Move downloaded model to model_dir\n",
    "    for dir, contents, files in os.walk(path):\n",
    "        # Create sub-directory\n",
    "        if not os.path.exists(model_dir+dir[len(path):]):\n",
    "            os.mkdir(model_dir+dir[len(path):])\n",
    "        # Move files\n",
    "        for file in files:\n",
    "            os.rename(dir+'/'+file, model_dir+dir[len(path):]+'/'+file)\n",
    "    print('Model downloaded succesfully.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5dbc36df-a538-4f9b-a020-391aa705c2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is already on disk.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory for google nnlm models\n",
    "if not os.path.exists(home_dir+'/models/google_nnlm'):\n",
    "    os.mkdir(home_dir+'/models/google_nnlm')\n",
    "\n",
    "# Download Spanish text embedding model\n",
    "download_g_nnlm(\"google/nnlm/tensorFlow2/es-dim50-with-normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44d883c9-d7b4-49f5-8889-a9558e130d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of embedding vectors = 976640\n",
      "embedding dimension = 50\n"
     ]
    }
   ],
   "source": [
    "# You can load the embedding model via:\n",
    "#    hub.load(path) or tf.saved_model.load(path) as a tensorflow saved model.\n",
    "#    hub.KerasLayer(path, input_shape=[], dtype=tf.string) as a keras layer.\n",
    "embedding_es50wn = hub.KerasLayer(home_dir+'/models/google_nnlm/es50_wn')\n",
    "\n",
    "num_embeddings_es50wn, embedding_dim_es50wn = embedding_es50wn.variables[0].shape\n",
    "\n",
    "print(f'number of embedding vectors = {num_embeddings_es50wn}')\n",
    "print(f'embedding dimension = {embedding_dim_es50wn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c200d-fdeb-440f-a5d6-3331d27f968b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pip]",
   "language": "python",
   "name": "conda-env-pip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
