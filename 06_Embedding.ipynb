{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5caeb90-90db-465a-acb2-9ec1e6097d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "924e7859-0a67-49f4-acb9-de330eb082bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign home_dir as a local directory to use when saving data to disk.\n",
    "home_dir = os.getenv(\"home_BA885\")\n",
    "\n",
    "if not home_dir:\n",
    "    home_dir = os.getcwd()\n",
    "# Colab home\n",
    "# home_dir = \"/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00e78f-5cd6-4109-aeb2-b8ee1b6becbc",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "Author: Nima Doroud\n",
    "\n",
    "The most publicized ML/AI models are Natural Language Processing (NLP) models including Neural Machine Translation (ex: Google Translate) and Next-word Prediction (ex: Auto-complete and Large Language Models). Furthermore, most real-world models deal with multiple types (or modes) of data including text inputs. For instance, when building a recommendation model, we include item description or user reviews as inputs.\n",
    "\n",
    "For a neural network to process natural language, we first need a map that transforms text into tensors with numeric values. To that end we need to standardize and tokenize the text. Once we have a list of all the unique tokens in our dataset, we can represent each token with a unique one-hot vector (vectorization) or dense vector (embedding). For example, the input text \"Hello World!\" transforms as follows:\n",
    "\n",
    "```\n",
    "\"Hello World!\" -> [\"hello\", \"world\"] -> [v[0], v[1]]\n",
    "```\n",
    "For a large vocabulary (number of unique tokens), there are a lot of relations (synonyms, antonyms, tense, ...) among the tokens that are not captured by a simple vectorization. Moreover, these relations are -- for the most part -- inherent to the language and can be learned from any corpus of text in that language. This is the role of embedding models.\n",
    "\n",
    "To see the inner workings of embedding models, we will build and train a Word2Vec embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6049080-9562-40e4-adf0-0c30c767abf8",
   "metadata": {},
   "source": [
    "## IMDB reviews\n",
    "\n",
    "We will use the IMDB reviews dataset which can be found <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\"> here </a> and is also available through TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f280d28-437f-40a5-9320-6c2bb729c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "if not os.path.exists(home_dir+'/datasets/aclImdb'):\n",
    "    ds_link='https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    tf.keras.utils.get_file('aclImdb_v1.tar.gz', \n",
    "                            ds_link, \n",
    "                            cache_dir='./',\n",
    "                            cache_subdir='datasets',\n",
    "                            extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4595d9d-8ee7-4942-8b54-9a239962d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for miscellaneous files (if you intend to load\n",
    "# the entire dataset)\n",
    "if not os.path.exists(home_dir+'/datasets/aclImdb/train_miscellaneous'):\n",
    "    os.mkdir(home_dir+'/datasets/aclImdb/train_miscellaneous')\n",
    "\n",
    "# Move miscellaneous files\n",
    "for x in os.scandir(home_dir+'/datasets/aclImdb/train'):\n",
    "    if x.is_file():\n",
    "        os.rename(home_dir+'/datasets/aclImdb/train/'+x.name,\n",
    "                  home_dir+'/datasets/aclImdb/train_miscellaneous/'+x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4f9228-4788-4ac1-96d9-3636980cd22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the 'unsupervised' reviews as a dataset\n",
    "batch_size = 128\n",
    "ds_dir = home_dir+'/datasets/aclImdb/train/unsup'\n",
    "\n",
    "# Strip html elements\n",
    "def strip_html(input_str):\n",
    "    ''' Strips the all occurrences of the string \"<br />\" from\n",
    "    the dataset. Use with text_ds.map.\n",
    "    '''\n",
    "    return tf.strings.regex_replace(input_str, \"<br />\", \" \")\n",
    "\n",
    "# Parallelization parameter of dataset.map\n",
    "num_cores = 12\n",
    "\n",
    "# Create the dataset from the reviews ignoring the categorization\n",
    "ds = keras.utils.text_dataset_from_directory(\n",
    "    directory = ds_dir,\n",
    "    labels = None,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = None,\n",
    "    subset = None,\n",
    "    seed = 1).map(strip_html, num_parallel_calls = num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13def06c-43e3-4286-88c7-59b133470451",
   "metadata": {},
   "source": [
    "Note: You can also import the dataset via tensorflow_datasets:\n",
    "```\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#tfds.__version__ == '4.9.6'\n",
    "\n",
    "ds_train, ds_test, ds_unsup = tfds.load(name=\"imdb_reviews\",\n",
    "          split=['train', 'test', 'unsupervised'],\n",
    "          batch_size=-1,\n",
    "          data_dir=home_dir,\n",
    "          as_supervised=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7cbd2f-2439-4952-83cd-8ee449abdd1e",
   "metadata": {},
   "source": [
    "## Pre-trained embedding\n",
    "\n",
    "For most applications, you can simply choose a pre-trained model from a plethora of available text embedding models such as Google's Neural-Net Language Models (NNLM) which are trained on Google News datasets. Using a pre-trained embedding provides an accurate representation of (common) text data to your model and can reduce the time and resources required to train and fine-tune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08bb840f-4751-474a-85c2-19d0ef041f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to download a google NNLM model\n",
    "def download_g_nnlm(nnlm_model: str):\n",
    "    '''Downloads the NNLM model from Google's NNLM models on kaggle\n",
    "    (https://www.kaggle.com/models/google/nnlm) where pretrained\n",
    "    embedding models in multiple languages are available. You can\n",
    "    choose the embedding dimension to be 50 or 128. Finally, you can\n",
    "    choose models with or without text normalization to remove \n",
    "    punctuations.\n",
    "\n",
    "    The downloaded model is moved to:\n",
    "        home_dir/models/google_nnlm/lled(wn)\n",
    "        ll: 2 letter abbreviation of the language (en, es, de,...)\n",
    "        ed: embedding dimension (50 or 128)\n",
    "        wn: with normalization\n",
    "    '''\n",
    "    # model specs\n",
    "    model = nnlm_model.split('/')[-1]\n",
    "    language = model.split('-')[0]\n",
    "    embedding_dim = model.split('-')[1].split('dim')[-1]\n",
    "    model_dir = f'{home_dir}/models/google_nnlm/{language}{embedding_dim}'\n",
    "    if len(model.split('-'))>2:\n",
    "        model_dir += '_wn'\n",
    "    # Check if the model is on disk\n",
    "    if os.path.exists(model_dir):\n",
    "        print('Model is already on disk.')\n",
    "        return None\n",
    "    # download model\n",
    "    path = kagglehub.model_download(nnlm_model, force_download=True)\n",
    "    # Create model_dir\n",
    "    os.mkdir(model_dir)\n",
    "    # Move downloaded model to model_dir\n",
    "    for dir, contents, files in os.walk(path):\n",
    "        # Create sub-directory\n",
    "        if not os.path.exists(model_dir+dir[len(path):]):\n",
    "            os.mkdir(model_dir+dir[len(path):])\n",
    "        # Move files\n",
    "        for file in files:\n",
    "            os.rename(dir+'/'+file, model_dir+dir[len(path):]+'/'+file)\n",
    "    print('Model downloaded succesfully.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc6beef-282e-4be1-83f8-b8275987cc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is already on disk.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory for google nnlm models\n",
    "if not os.path.exists(home_dir+'/models/google_nnlm'):\n",
    "    os.mkdir(home_dir+'/models/google_nnlm')\n",
    "\n",
    "# Download English text embedding model\n",
    "download_g_nnlm(\"google/nnlm/tensorFlow2/en-dim50-with-normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e23fd4a-29d5-422f-8684-f0cde12d4582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of embedding vectors = 963812\n",
      "embedding dimension = 50\n"
     ]
    }
   ],
   "source": [
    "# You can load the embedding model via:\n",
    "#    hub.load(path) or tf.saved_model.load(path) as a tensorflow model\n",
    "#    hub.KerasLayer(path, input_shape=[], dtype=tf.string) as a keras layer.\n",
    "embedding_en50wn = hub.load(home_dir+'/models/google_nnlm/en50_wn')\n",
    "\n",
    "num_embeddings_en50wn, embedding_dim_en50wn = embedding_en50wn.embeddings.shape\n",
    "\n",
    "print(f'number of embedding vectors = {num_embeddings_en50wn}')\n",
    "print(f'embedding dimension = {embedding_dim_en50wn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035012c5-c62e-444d-8c5f-3b474cb6c9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample string \"Fairly entertaining movie, but ...\"\n",
      " is embedded as:\n",
      "[[0.667886376 0.0443405434 -0.149116173 ... -0.410180092 -0.107391208 -0.155435026]\n",
      " [0.302538723 0.198385894 -0.0855945349 ... -0.781784713 0.252008289 0.321103722]\n",
      " [0.182023913 0.232008681 0.136273429 ... -0.215836763 0.132544205 -0.0584928058]\n",
      " [0.21110025 0.314057738 -0.119848102 ... -0.0213225503 0.447388232 -0.0899484828]]\n"
     ]
    }
   ],
   "source": [
    "# Here is the output of the embedding on a sample string\n",
    "for x in ds.take(1):\n",
    "    i = 4\n",
    "    sample_embedding = embedding_en50wn(x.numpy()[:i])\n",
    "    sample_string = x.numpy()[0].decode('utf-8').split(' ')[:i]\n",
    "    sample_string = np.reshape(\n",
    "        np.transpose(\n",
    "            np.concatenate(\n",
    "                [[sample_string],[[' ']*len(sample_string)]], axis=0\n",
    "            )\n",
    "        ), (-1))\n",
    "    sample_string = ''.join(sample_string[:-1])\n",
    "    print(f'Sample string \"{sample_string} ...\"\\n is embedded as:')\n",
    "    tf.print(sample_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1f102-1561-47e1-a01d-6ff1be952f43",
   "metadata": {},
   "source": [
    "### Embedding vocabulary\n",
    "\n",
    "In order to translate embedding vectors back to tokens, we need to form the embedding vocabulary. This takes the form of a dictionary {index : token} where the index identifies the embedding vector from our set of 963812 embedding vectors that comprise the weights of the embedding model and the token is the token string corresponding to that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8fac177-8bcd-4efd-ba45-aa3beedeac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate embedding vocabulary\n",
    "def generate_nnlm_vocab(model_path: str,\n",
    "                        embedding,\n",
    "                        unmatched_indices = [],\n",
    "                        unmatched_tokens = [],\n",
    "                        populate_unmatched = True) -> dict:\n",
    "    '''Generates a dictionary for a pretrained Google NNLM embedding\n",
    "    model to map embedding vector indices to token strings: \n",
    "        vocab[i] = token  <-->  embedding(token) = weights[i]\n",
    "    Returns a dictionary.\n",
    "\n",
    "    Inputs:\n",
    "        model_path: A string specifying the path of the model\n",
    "        Ex: .../models/google_nnlm/en50_wn\n",
    "        \n",
    "        embedding: The model pre-loaded as a keras model\n",
    "        \n",
    "        unmatched_indices: An empty list which will be updated\n",
    "        with vector indices that could not be matched with\n",
    "        token strings in the tokens file.\n",
    "\n",
    "        unmatched_tokens: An empty list which will be updated\n",
    "        to store token strings whose embedding is not an exact\n",
    "        match with the embedding vectors.\n",
    "\n",
    "        populate_unmatched: When False, the output dictionary\n",
    "        will not have unmatched indices as keys.\n",
    "        When True, dict.keys = range(num_embeddings).\n",
    "    '''\n",
    "    num_embeddings, embedding_dim = embedding.embeddings.shape\n",
    "    # Load the list of token strings from the tokens file\n",
    "    token_strings = []\n",
    "    with open(model_path+'/assets/tokens.txt','r') as tokens:\n",
    "        for t in tokens:\n",
    "            token_strings.append(t.strip('\\n'))\n",
    "    # Convert to numpy array\n",
    "    token_strings = np.array(token_strings).astype(str)\n",
    "    # Evaluate the embedding vectors associated to the tokens\n",
    "    token_embeddings = embedding(token_strings)\n",
    "    # Pad token embeddings to match embedding.weights[0].shape\n",
    "    token_embeddings = tf.pad(token_embeddings,\n",
    "                              [[0,num_embeddings-len(token_strings)],[0,0]],\n",
    "                              \"CONSTANT\")\n",
    "    # Find the indices of tokens with matching embedding vectors.\n",
    "    # i.e. (index, token) such that embedding(token) == weights[i]\n",
    "    matched_indices = tf.squeeze(\n",
    "        tf.where(\n",
    "            tf.math.reduce_all(token_embeddings == embedding.embeddings,\n",
    "                               axis=1)\n",
    "        )).numpy()\n",
    "    # Create a dictionary to store the index:token pairs\n",
    "    vocabulary = pd.DataFrame(token_strings[matched_indices],\n",
    "                              index = matched_indices).to_dict()[0]\n",
    "    # Add the indices of unmatched embedding vectors to unmatched_indices\n",
    "    unmatched_indices += np.delete(np.arange(num_embeddings), matched_indices).tolist()\n",
    "    # Add unmatched token strings to unmatched_tokens\n",
    "    unmatched_tokens += np.delete(token_strings, matched_indices).tolist()\n",
    "    # Populate the unmatched indices in the vocabulary\n",
    "    if populate_unmatched:\n",
    "        vocabulary.update({i:f'#UNK_{i}' for i in unmatched_indices})\n",
    "    #\n",
    "    print(f'Number of embedding vectors = {num_embeddings}')\n",
    "    print(f'Number of tokens = {len(token_strings)}')\n",
    "    print(f'Number of matched (index,token) pairs = {len(matched_indices)}')\n",
    "    print(f'Number of unmatched embedding vectors = {len(unmatched_indices)}')\n",
    "    print(f'Number of unmatched token strings = {len(unmatched_tokens)}')\n",
    "    #\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfbdf8d5-6c68-410f-bb00-1a28953e8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embedding vectors = 963812\n",
      "Number of tokens = 960752\n",
      "Number of matched (index,token) pairs = 899290\n",
      "Number of unmatched embedding vectors = 64522\n",
      "Number of unmatched token strings = 61462\n"
     ]
    }
   ],
   "source": [
    "# Create two numpy arrays to store unmatched indices and token strings\n",
    "unmatched_indices_en50wn = []\n",
    "unmatched_tokens_en50wn = []\n",
    "\n",
    "# Generate the embedding model vocabulary\n",
    "vocabulary_en50wn = generate_nnlm_vocab(home_dir+'/models/google_nnlm/en50_wn',\n",
    "                                        embedding_en50wn,\n",
    "                                        unmatched_indices_en50wn,\n",
    "                                        unmatched_tokens_en50wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e7ecea-01e0-419c-b130-cee6c2d45687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</S>', ',', '.', '\"', '-', \"'s\", '##', ')', '(', '####']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unmatched tokens will include punctuations (since we are\n",
    "# using the model with normalization) as well as infrequent tokens\n",
    "# which are grouped into hash buckets.\n",
    "unmatched_tokens_en50wn[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b532f2-55dc-4bf8-af34-c17d9cf3cae0",
   "metadata": {},
   "source": [
    "### Embedding distance\n",
    "\n",
    "Generative NLP models, such as large language models (LLM) first generate a vector, V, in the embedding space. In order to match V to a token we need to find the token with the 'closest' embedding vector to V. Thus, we need a notion of distance or similarity in our embedding space. The two common choices are the Euclidean distance (keras.losses.MeanSquaredError) and cosine similarity (keras.losses.cosine_similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3149bc2d-cc3e-4426-99b6-9e9afc62d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the k nearest neighboring embedding\n",
    "# vectors of a point in the embedding space.\n",
    "def token_knn_en50wn(v: tf.Tensor, k=1) -> pd.DataFrame:\n",
    "    ''' For a point in the embedding space identified by the\n",
    "    50-component tensor, v, finds the k nearest token indices\n",
    "    identified by token embeddings of the google nnlm en50 --\n",
    "    with normalization -- model.\n",
    "\n",
    "    Inputs:\n",
    "        v: a tf.Tensor of shape (50,) or (1,50).\n",
    "        k: an integer > 0\n",
    "    Returns a dataframe with the index column listing the nearby\n",
    "    tokesn and the distance column their distance from the input\n",
    "    vector.\n",
    "    '''\n",
    "    x = tf.math.reduce_sum(\n",
    "        tf.square(tf.subtract(embedding_en50wn.embeddings, v)),\n",
    "        axis=1).numpy()\n",
    "    x = pd.DataFrame(x).sort_values(by=0).rename(columns={0: 'distance'}).iloc[:k]\n",
    "    x.index = x.index.map(vocabulary_en50wn)\n",
    "    x.distance = np.sqrt(x.distance)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ce4fde8-1c22-4ea8-8127-ae49262636ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 closest tokens to \"bike\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bike</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bicycle</th>\n",
       "      <td>0.585474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorbike</th>\n",
       "      <td>0.671201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skateboard</th>\n",
       "      <td>0.678404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scooter</th>\n",
       "      <td>0.685947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            distance\n",
       "bike        0.000000\n",
       "bicycle     0.585474\n",
       "motorbike   0.671201\n",
       "skateboard  0.678404\n",
       "scooter     0.685947"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use this function to study very basic word relationships:\n",
    "sample_token = 'bike'\n",
    "k = 5\n",
    "print(f'The {k} closest tokens to \"{sample_token}\":')\n",
    "token_knn_en50wn(embedding_en50wn([sample_token]), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b63b4-5cad-4c6a-b095-99fdfcdbefcf",
   "metadata": {},
   "source": [
    "## Custom embedding\n",
    "\n",
    "A pre-trained embedding has a fixed tokenization which is most appropriate for its training data. Thus, if you want to choose your own tokens or if your dataset has uncommon tokens, you may need to build your own embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e982fb-d160-401b-9633-ec412b285966",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Our first task is standardize the data. To minimize the number of tokens we convert all the text into lower case and remove all symbols and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b452302-59c1-4040-a858-e325997ab13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following charachters will be striped: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Create a string of all the charachters to strip from the dataset.\n",
    "strip_chars = string.punctuation\n",
    "\n",
    "print(f'The following charachters will be striped: {strip_chars}')\n",
    "\n",
    "# Escape the special charachters via the regular expression package.\n",
    "# The square brackets indicate that there are multiple patterns.\n",
    "strip_chars = '['+re.escape(strip_chars)+']'\n",
    "\n",
    "# Define a standardization function for the text dataset.\n",
    "def custom_standardization(input_str):\n",
    "    '''Standardizes an input string or tensor of strings by\n",
    "    convering all the charachters to lower case and striping\n",
    "    the charachters in strip_chars. Returns a string valued\n",
    "    tf.tensor.\n",
    "    '''\n",
    "    return tf.strings.regex_replace(tf.strings.lower(input_str), strip_chars, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c10182-7ec1-4876-9eed-1335159f9345",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Next we need to convert the standardized words/tokens into vectors. We can simply list all the unique words as our vocabulary and use the index or the one-hot vectors, but in a large corpus of text the number of unique words may be too large. Thus, to limit the size of the vocabulary we also need to consider the frequency of appearance of each word and only drop the least frequent words.\n",
    "\n",
    "We can achieve this using the TextVectorization layer in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e969ccc-3efa-442d-bc41-ded1db1a0770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 172830 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count the number of unique tokens in a dataset\n",
    "def count_unique_tokens(ds: tf.data.Dataset) -> int:\n",
    "    ''' Counts the number of unique words/tokens in a\n",
    "    string-valued dataset.\n",
    "    '''\n",
    "    unique_tokens = np.array([], dtype=str)\n",
    "    for batch in ds:\n",
    "        batch_tokens = np.array([], dtype=str)\n",
    "        for s in batch.numpy():\n",
    "            batch_tokens = np.concatenate([batch_tokens, s.decode(\"utf-8\").split(' ')])\n",
    "            batch_tokens = np.unique(batch_tokens)\n",
    "        unique_tokens = np.unique(np.concatenate([unique_tokens, batch_tokens]))\n",
    "    return unique_tokens.shape[0]\n",
    "\n",
    "# Find the number of unique tokens in the dataset\n",
    "num_unique_tokens = count_unique_tokens(ds.map(custom_standardization))\n",
    "print(f'Found {num_unique_tokens} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82a09ee1-8ea7-4925-bac9-19588161bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the vast majority of the unique tokens we counted\n",
    "# appear only a handful of times.\n",
    "# Set the size of the vocabulary\n",
    "imdb_vocab_size = 120000\n",
    "\n",
    "# Create a vectorization layer for the dataset.\n",
    "imdb_vectorization = layers.TextVectorization(\n",
    "    max_tokens=imdb_vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    ragged=True,\n",
    "    standardize=custom_standardization)\n",
    "\n",
    "# Load or adapt the vectorization layers vocabularies.\n",
    "if os.path.exists(home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}.csv'):\n",
    "    imdb_vectorization.set_vocabulary(list(pd.read_csv(\n",
    "        home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}.csv',\n",
    "        header=None,\n",
    "        na_filter='')[0]))\n",
    "else:\n",
    "    imdb_vectorization.adapt(ds)\n",
    "\n",
    "# Save vocabulary to disk\n",
    "if not os.path.exists(home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}.csv'):\n",
    "    pd.DataFrame(imdb_vectorization.get_vocabulary()).to_csv(\n",
    "        home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}.csv',\n",
    "        header=False,\n",
    "        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0f4fb46-ff52-4a51-9611-e8b6b94a07f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sample string \"Fairly entertaining movie, but ...\" vectorizes as: \n",
      " \n",
      "[[930, 392, 17, 18]]\n"
     ]
    }
   ],
   "source": [
    "# The vectorization layer takes a string (review from our dataset)\n",
    "# as input and generates an integer-valued tensor as the output.\n",
    "# The values can be interpreted as the index of each word in the corresponding vocabulary.\n",
    "print(f'\\nThe sample string \"{sample_string} ...\" vectorizes as: \\n ')\n",
    "tf.print(imdb_vectorization(np.array([sample_string])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80e932e6-5141-43eb-bf21-179581657662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectors can be reverted back into text via a dictionary that maps\n",
    "# the indices to the vocabulary.\n",
    "imdb_vocabulary = imdb_vectorization.get_vocabulary()\n",
    "imdb_vocabulary = dict(zip(range(len(imdb_vocabulary)), imdb_vocabulary))\n",
    "\n",
    "def decode_imdb_vector(v: np.array) -> str:\n",
    "    '''Transforms a vectorized imdb review back to\n",
    "    English text using the recorded vocabulary.\n",
    "    Returns a string.\n",
    "    '''\n",
    "    tokens = list(map(imdb_vocabulary.get, v))\n",
    "    tokens = np.reshape(np.transpose(np.concatenate([[tokens], [[' ']*v.shape[-1]]])), (-1))\n",
    "    return ''.join(tokens).rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb3ac8f-13a9-4592-86f1-e9947840d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Fairly entertaining movie, but ...\n",
      "\n",
      "Decoded text:  fairly entertaining movie but\n"
     ]
    }
   ],
   "source": [
    "# Here is an example for our decoder:\n",
    "print(f'Original text: {sample_string} ...\\n')\n",
    "print('Decoded text: ',decode_imdb_vector(np.array([930, 392, 17, 18])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b4dca3-4537-417f-b2aa-144ccbe50a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index corresponding to the word \"bike\" is 6866\n",
      "The index corresponding to the word \"scooter\" is 24990\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Note that the only information the vectorization layers encode is the \n",
    "frequency of appearance of words in our dataset. Word relationships \n",
    "such as synonymity are not encoded. To illustrate this, consider the \n",
    "synonymous words \"bike\" and \"scooter\".\n",
    "'''\n",
    "# Indices of synonymous words\n",
    "print('The index corresponding to the word \"bike\" is {}'.format(\n",
    "    imdb_vectorization(np.array(['bike'])).numpy()[0][0]))\n",
    "print('The index corresponding to the word \"scooter\" is {}'.format(\n",
    "    imdb_vectorization(np.array(['scooter'])).numpy()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f694005-8cdc-46dd-92eb-587e3f8ab1cc",
   "metadata": {},
   "source": [
    "### Word2Vec embedding\n",
    "\n",
    "<a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\">Word2Vec</a> is a class of neural network models proposed to generate an effective embedding by analyzing the pairing of words in a corpus of text. To illustrate the method by which the embedding vectors are generated consider the following simple model.\n",
    "\n",
    "We start with an embedding space and assign two random vectors, u and v, to each token in our vocabulary. The vectors u[i] will serve as the embedding vectors while the vectors v[i] are auxiliary vectors that will capture 'compatibility'. If two words appear together in our dataset (ex: \"entertaining\" and \"movie\") we consider them compatible. Correspondingly, the model determines compatibility of the two words based on the inner product of their embedding vectors v[i].u[j]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6798d-ad94-41fd-84a0-116a15bf3bc1",
   "metadata": {},
   "source": [
    "#### Positive skipgrams\n",
    "\n",
    "To train our model, we need to generate a dataset of pairs of neighboring words. We can further improve the model by including next to nearest words and so on, but for this simple example we will only consider adjacent words or 'bigrams'. (You can also generate such a dataset using keras.preprocessing.sequence.skipgrams.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c28c3480-ee1d-42ab-bc6c-79d63a5e6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to break text into sentences.\n",
    "def break_into_sentences(batch: tf.RaggedTensor, sep=['. ', '; ']) -> tf.RaggedTensor:\n",
    "    ''' Breaks down a string valued dataset into sentences using the\n",
    "    markers in 'sep'. Returns a ragged tensor of the same dimension with\n",
    "    the sentences stacked along axis=0.\n",
    "    Use with dataset.map().\n",
    "    '''\n",
    "    x = batch\n",
    "    for s in sep:\n",
    "        x = tf.strings.split(x, s)\n",
    "        x = x.merge_dims(0,1)\n",
    "    return x\n",
    "\n",
    "# Define a function to add markers at the two ends of each sentence\n",
    "def add_markers(batch: tf.RaggedTensor) -> tf.RaggedTensor:\n",
    "    ''' Adds the markers \"[start]\" and \"[end]\" to the beginning and\n",
    "    the end of each setence in the dataset.\n",
    "    Use with dataset.map().\n",
    "    '''\n",
    "    m0 = tf.fill(batch.shape, '[start]')\n",
    "    m1 = tf.fill(batch.shape, '[end]')\n",
    "    return tf.strings.join([m0, batch, m1], separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07fa3780-7bc9-4a48-ad5c-fa9245a8d7d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a copy of the dataset and break it down into sentences.\n",
    "ds_b = ds.map(break_into_sentences, num_parallel_calls = num_cores)\n",
    "\n",
    "# Standardize the dataset\n",
    "ds_b = ds_b.map(custom_standardization, num_parallel_calls = num_cores)\n",
    "\n",
    "# Add markers to mark the beginning and the end of each sentence.\n",
    "ds_b = ds_b.unbatch().map(\n",
    "    add_markers, num_parallel_calls = num_cores).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24021a9b-9471-481f-b14c-62c9a213cb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our new vocabulary includes the markers:  [(0, ''), (1, '[UNK]'), (2, 'the'), (3, '[start]'), (4, '[end]')]\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize the vectorization layer without standardization\n",
    "imdb_vectorization = layers.TextVectorization(\n",
    "    max_tokens=imdb_vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    ragged=True,\n",
    "    standardize=None)\n",
    "\n",
    "# Reload or re-adapt the vectoriazation layer to include the markers.\n",
    "if os.path.exists(home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}_wm.csv'):\n",
    "    imdb_vectorization.set_vocabulary(list(pd.read_csv(\n",
    "        home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}_wm.csv',\n",
    "        header=None,\n",
    "        na_filter='')[0]))\n",
    "else:\n",
    "    imdb_vectorization.adapt(ds_b)\n",
    "\n",
    "# Save vocabulary to disk\n",
    "if not os.path.exists(home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}_wm.csv'):\n",
    "    pd.DataFrame(imdb_vectorization.get_vocabulary()).to_csv(\n",
    "        home_dir+f'/datasets/aclImdb/imbd_vocab_{imdb_vocab_size}_wm.csv',\n",
    "        header=False,\n",
    "        index=False)\n",
    "\n",
    "# Update the vocabulary\n",
    "imdb_vocabulary = imdb_vectorization.get_vocabulary()\n",
    "imdb_vocabulary = dict(zip(range(len(imdb_vocabulary)), imdb_vocabulary))\n",
    "\n",
    "# Print first few tokens in vocabulary\n",
    "print('Our new vocabulary includes the markers: ',\n",
    "      list(zip(range(5), map(imdb_vocabulary.get, range(5)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcb91893-d270-449f-a8e1-c6599c21910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate bigrams\n",
    "def gen_bigrams(batch: tf.RaggedTensor) -> tf.RaggedTensor:\n",
    "    ''' Generates bigrams -- pairs of adjacent tokens in the\n",
    "    dataset -- from a (ragged) tensor of sentences (strings).\n",
    "    Returns a Ragged Tensor of bigrams (strings with 2 tokens).\n",
    "    Use with dataset.map().\n",
    "    '''\n",
    "    x = tf.strings.split(batch, ' ')\n",
    "    x = tf.strings.join([x[:, :-1], x[:, 1:]], separator=' ')\n",
    "    x = x.merge_dims(0,1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e6e2672-1456-4558-ae65-03d419242fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the sentences dataset into bigrams\n",
    "ds_b = ds_b.map(gen_bigrams, num_parallel_calls = num_cores)\n",
    "\n",
    "# Vectorize the bigrams using our vectorization layer\n",
    "ds_b = ds_b.map(imdb_vectorization, num_parallel_calls = num_cores)\n",
    "\n",
    "# Convert RaggedTensor to normal Tensor\n",
    "ds_b = ds_b.map(\n",
    "    lambda rt: tf.reshape(rt.flat_values, (-1,2)),\n",
    "    num_parallel_calls = num_cores)\n",
    "\n",
    "# Re-batch the dataset\n",
    "ds_b = ds_b.unbatch().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ca4ee6a-75bf-4bfa-92ee-70f5d4cb47d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a generator that generates 1.\n",
    "def one_gen():\n",
    "    yield 1\n",
    "\n",
    "# Create the positive labels dataset\n",
    "ds_pos_labels = tf.data.Dataset.from_generator(\n",
    "    one_gen,\n",
    "    output_signature=(tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
    "\n",
    "# Repeat and batch the labels dataset\n",
    "ds_pos_labels = ds_pos_labels.repeat().batch(batch_size)\n",
    "\n",
    "# Combine the bigrams and labels dataset to return (bigram, label) pairs\n",
    "ds_b = tf.data.Dataset.zip((ds_b, ds_pos_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f08bf8-c453-45ee-bf23-f5e60d6b152f",
   "metadata": {},
   "source": [
    "#### Negative skipgrams\n",
    "\n",
    "So far we only have a dataset of positive (compatible) skipgrams. We can also include negative skipgrams to help better train our embedding model. We will generate these at random.\n",
    "\n",
    "Note that of all possible bigrams (1.44e10) the vast majority are negative (incompatible) bigrams. Thus, while generating negative bigrams at random does produce 'false negatives', they do not hinder the training of our embedding model. Furthermore, the more compatible a pair of tokens are, the more frequently the appear in the positive bigrams dataset thus reducing the impact of such false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78dddc0a-b67c-40bb-87d6-ffc6b5125690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of negative samples\n",
    "num_neg = int(1e8)\n",
    "\n",
    "# Define a function to produce random bigrams with label=0\n",
    "def bigrams_gen(size = num_neg, vocab_size = imdb_vocab_size):\n",
    "    gen = tf.random.Generator.from_seed(1)\n",
    "    return (gen.uniform(shape=(size,2), minval=0, maxval=vocab_size, dtype=tf.int64),\n",
    "           tf.zeros(shape=(size,), dtype=tf.int32))\n",
    "\n",
    "# Create a dataset of negative/random bigrams\n",
    "ds_neg = tf.data.Dataset.from_tensor_slices(bigrams_gen()).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "498b083d-864c-4940-ab30-90a72372585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the dataset from the combined positive and negative skipgrams datasets\n",
    "ds_b = tf.data.Dataset.sample_from_datasets([ds_b, ds_neg], weights=[0.5,0.5])\n",
    "\n",
    "# Increase batch size and re-batch the dataset\n",
    "batch_size = 256\n",
    "ds_b = ds_b.unbatch().batch(batch_size)\n",
    "\n",
    "# Shuffle, prefetch and chache\n",
    "ds_b = ds_b.shuffle(256).prefetch(128).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714360f-daad-4440-ab04-e81178c13912",
   "metadata": {},
   "source": [
    "#### Embedding model\n",
    "\n",
    "We can now implement our simple word2vec model and train it using the imdb skipgrams dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b28ef48-cbee-4c01-a5fa-a10f6d8a030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate our simple word2vec model\n",
    "def simple_word2vec_model(embedding_dim : int) -> keras.Model:\n",
    "    ''' Creates a simple word2vec model which takes pairs of integers,\n",
    "    which represent the index of tokens in imdb_vocabulary, as inputs\n",
    "    and measures their compatibility using their embeddings.\n",
    "    '''\n",
    "    x = layers.Input(shape = (2,))\n",
    "    x = layers.Reshape(target_shape = (2,1))(x)\n",
    "    # branch_0: embedding of the bigrams[:, 0] tokens\n",
    "    u = layers.Cropping1D(cropping = (0,1))(x)\n",
    "    u = layers.Reshape(target_shape = ())(u)\n",
    "    u = layers.Embedding(input_dim=imdb_vocab_size, output_dim=embedding_dim)(u)\n",
    "    # branch_1: Auxiliary embedding of the bigrams[:,1] tokens\n",
    "    v = layers.Cropping1D(cropping = (1,0))(x)\n",
    "    v = layers.Reshape(target_shape = ())(v)\n",
    "    v = layers.Embedding(input_dim=imdb_vocab_size, output_dim=embedding_dim)(v)\n",
    "    # Compatibility of the output of the two branches\n",
    "    y = layers.Dot(axes=(1,1))([u,v])\n",
    "    y = layers.Activation('sigmoid')(y)\n",
    "    return keras.Model(x, y, name = 'simple_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c81797d5-0a9a-42c4-90d1-cf5b5482a155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_word2vec\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 2, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 1, 1)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " cropping1d_1 (Cropping1D)      (None, 1, 1)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None,)              0           ['cropping1d[1][0]']             \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None,)              0           ['cropping1d_1[1][0]']           \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 48)           5760000     ['reshape_1[1][0]']              \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 48)           5760000     ['reshape_2[1][0]']              \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['embedding[1][0]',              \n",
      "                                                                  'embedding_1[1][0]']            \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 1)            0           ['dot[1][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,520,000\n",
      "Trainable params: 11,520,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile the model using the appropriate optimizer,\n",
    "# loss function and metrics.\n",
    "embed_dim = 48\n",
    "sw2v_model = simple_word2vec_model(embed_dim)\n",
    "\n",
    "sw2v_optimizer = \"adam\"\n",
    "sw2v_loss = \"binary_crossentropy\"\n",
    "sw2v_metrics = [\"binary_accuracy\"]\n",
    "\n",
    "sw2v_model.compile(optimizer=sw2v_optimizer, loss=sw2v_loss, metrics=sw2v_metrics)\n",
    "\n",
    "# Generate a summary of the model\n",
    "sw2v_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440dffb6-802a-41ea-af3c-ab275cd01850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model/load weights from file\n",
    "if not os.path.exists(home_dir+f'/models/06_embedding/simple_word2vec_{embed_dim}.h5'):\n",
    "    sw2v_history = sw2v_model.fit(ds_b,\n",
    "                                  batch_size=batch_size,\n",
    "                                  steps_per_epoch=1024,\n",
    "                                  epochs=4)\n",
    "else:\n",
    "    sw2v_model.load_weights(home_dir+f'/models/06_embedding/simple_word2vec_{embed_dim}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3da759b3-0933-46a0-9d27-25580d11f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to file\n",
    "if not os.path.exists(home_dir+'/models/06_embedding'):\n",
    "    os.mkdir(home_dir+'/models/06_embedding')\n",
    "\n",
    "if not os.path.exists(home_dir+f'/models/06_embedding/simple_word2vec_{embed_dim}.h5'):\n",
    "    sw2v_model.save_weights(home_dir+f'/models/06_embedding/simple_word2vec_{embed_dim}.h5')\n",
    "else:\n",
    "    print('File already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "107ca6d7-24a0-4505-a775-06815307eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights of the embedding layer as the embedding map\n",
    "embedding_map = sw2v_model.weights[0].numpy()\n",
    "\n",
    "# Define a function to map words to their embedding\n",
    "def word_embed(word: str) -> np.array:\n",
    "    return embedding_map[imdb_vectorization(word).numpy()[0]]\n",
    "\n",
    "# Define a function to find the k nearest neighboring embedding\n",
    "# vectors of a point in the embedding space.\n",
    "def token_knn_sw2v(v: tf.Tensor, k=1) -> pd.DataFrame:\n",
    "    ''' For a point in the embedding space identified by the\n",
    "    32-component tensor, v, finds the k nearest token indices\n",
    "    identified by token embeddings of sw2v_model.\n",
    "\n",
    "    Inputs:\n",
    "        v: a tf.Tensor of shape (32,) or (1,32).\n",
    "        k: an integer > 0\n",
    "    Returns a dataframe with the index column listing the nearby\n",
    "    tokesn and the distance column their distance from the input\n",
    "    vector.\n",
    "    '''\n",
    "    x = tf.math.reduce_sum(\n",
    "        tf.square(tf.subtract(embedding_map, v)),\n",
    "        axis=1).numpy()\n",
    "    x = pd.DataFrame(x).sort_values(by=0).rename(columns={0: 'distance'}).iloc[:k]\n",
    "    x.index = x.index.map(imdb_vocabulary)\n",
    "    x.distance = np.sqrt(x.distance)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a79386-46f4-4e73-8e76-05f8ce380ba8",
   "metadata": {},
   "source": [
    "Bear in mind that we have only trained the simplest word2vec model and with only a small specialized corpus of text. Thus we can only expect the model to capture word relationships in the context of sentiment analysis of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81f49505-8228-47a9-bdcb-92a2a67a6357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of frequent words (\"____ movie\"): \n",
      "\n",
      "          distance\n",
      "dramatic   0.00000\n",
      "strange    0.30421\n",
      "cheap      0.34318 \n",
      "\n",
      "             distance\n",
      "action       0.000000\n",
      "interesting  0.396115\n",
      "excellent    0.409452 \n",
      "\n",
      "Examples of infrequent words: \n",
      "\n",
      "            distance\n",
      "cat         0.000000\n",
      "lies        0.269113\n",
      "flashbacks  0.289725 \n",
      "\n",
      "        distance\n",
      "norway  0.000000\n",
      "caesar  0.275237\n",
      "rosss   0.276205 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Examples of frequent words (\"____ movie\"):' , '\\n')\n",
    "for word in ['dramatic', 'action']:\n",
    "    print( token_knn_sw2v(word_embed(word), 3) ,'\\n' )\n",
    "\n",
    "print('Examples of infrequent words:' , '\\n')\n",
    "for word in ['cat', 'norway']:\n",
    "    print( token_knn_sw2v(word_embed(word), 3) ,'\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387abe0-6a63-4d44-812d-a34f109576d3",
   "metadata": {},
   "source": [
    "### Aside: Sparse tensors\n",
    "\n",
    "Many of the operations in this notebook involve sparsely populated tensors and can be replicated or are implicitly done using tf.sparse.SparseTensor objects. It is therefore beneficial to familiarize yourself with sparse tensors and sparse operations. We will discuss two examples here: Sparse one-hot vector and Sparse bigrams dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d12186-b7af-4107-9e20-a0ddad467c83",
   "metadata": {},
   "source": [
    "#### Sparse one-hot vectors\n",
    "\n",
    "While the keras embedding layer maps token index to embedding vector, the weights of the embedding layer have the following shape: (vocabulary_size, embedding_dim). To achieve the same effect using a dense layer, there is an intermediate step which is to map token index to a one-hot vector. The efficient way to store the one-hot vectors with a large number of classes is to use sparse tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d16c8ad9-d1a3-4aa6-84fa-44bd65ce36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate one-hot encodings as sparse tensors.\n",
    "def sparse_one_hot(indices: tf.Tensor, num_classes: tf.int64) -> tf.SparseTensor:\n",
    "    ''' Generates the one-hot encodings from the 'indices' array in\n",
    "    the form of a sparse tensor. It is the sparse equivalent of tf.one_hot.\n",
    "\n",
    "    indices: A numpy array of integers valued in the range(0,num_classes)\n",
    "    num_classes: Number of classes which determines the shape of the output\n",
    "    tensor along axis=1.\n",
    "    '''\n",
    "    sample_size = indices.__len__()\n",
    "    indices = tf.cast(indices, dtype=tf.int64)\n",
    "    indices = tf.concat([tf.expand_dims(tf.range(sample_size, dtype=tf.int64), axis=1),\n",
    "                         tf.expand_dims(indices, axis=1)], axis=1)\n",
    "    return tf.SparseTensor(indices = indices,\n",
    "                           values = tf.ones(shape=(sample_size,), dtype=tf.int64),\n",
    "                           dense_shape = (sample_size, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "562b2d4d-d9f3-48e1-9c8e-c62bf408bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sparse one-hot encoding of our sample review: \n",
      "\n",
      "'SparseTensor(indices=[[0 1027]\n",
      " [1 238]\n",
      " [2 2]\n",
      " ...\n",
      " [632 55]\n",
      " [633 57]\n",
      " [634 586]], values=[1 1 1 ... 1 1 1], shape=[635 120000])'\n"
     ]
    }
   ],
   "source": [
    "# Take a sample review from the imdb reviews dataset.\n",
    "for x in ds.take(1).map(custom_standardization):\n",
    "    sample_review = x.numpy()[0]\n",
    "\n",
    "# Vectorize the review using the imdb_vectorization layer and\n",
    "# Convert to sparse one-hot encoding\n",
    "sample_review_1h = imdb_vectorization(sample_review)\n",
    "sample_review_1h = sparse_one_hot(sample_review_1h, num_classes=imdb_vocab_size)\n",
    "\n",
    "# Print one-hot encodings\n",
    "print('Here is the sparse one-hot encoding of our sample review: \\n')\n",
    "tf.print(sample_review_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36b7a3d7-1b31-4b07-bd45-8d5c2f3fafd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(635, 64), dtype=float32, numpy=\n",
       "array([[ 0.23788725, -0.2656307 , -0.23392057, ...,  0.29518747,\n",
       "         0.2782097 , -0.21971081],\n",
       "       [ 0.35165983, -0.45316482, -0.30913842, ...,  0.44955376,\n",
       "         0.48651364, -0.43001372],\n",
       "       [-0.33992714, -1.0376357 , -1.0656841 , ...,  1.0289359 ,\n",
       "         1.0317237 , -1.1334589 ],\n",
       "       ...,\n",
       "       [ 0.38555515, -0.49675035, -0.41708922, ...,  0.37819645,\n",
       "         0.5308759 , -0.4329502 ],\n",
       "       [ 0.43772763, -0.547649  , -0.5353722 , ...,  0.57603467,\n",
       "         0.5433307 , -0.5437628 ],\n",
       "       [ 0.09001058, -0.19188467, -0.17871836, ...,  0.23232064,\n",
       "         0.21456173, -0.21309493]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When the inputs are sparse tensors we need to use the Input layer with\n",
    "# \"sparse = True\".\n",
    "embedding_model_s1h = keras.Sequential([\n",
    "    layers.Input(shape=(imdb_vocab_size), sparse=True),\n",
    "    layers.Dense(units=embed_dim, use_bias=False)],\n",
    "    name='Embedding_sparse_1h')\n",
    "\n",
    "# Load the trained weights from our simple w2v model\n",
    "embedding_model_s1h.set_weights([embedding_map])\n",
    "\n",
    "# Find the embedding of the sample review\n",
    "embedding_model_s1h(sample_review_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d90a28-666b-4846-a6c6-78b903d4a56f",
   "metadata": {},
   "source": [
    "#### Bigrams dataset as a sparse tensor\n",
    "\n",
    "Our bigrams dataset is too large to load as a single tensor. The main reason for this is the multiplicity of the bigrams, some of the bigrams appear thousands of times in the dataset!\n",
    "\n",
    "If we count the number of unique ([a,b] != [b,a]) bigrams with vocabulary size = 120000 we get 1.44e10 bigrams. Combined with the fact that the vast majority of these bigrams are negative bigrams, we can easily load the positive bigrams dataset as a single sparse tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75020dde-ab74-4228-8b9f-86817371b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the positive bigrams dataset\n",
    "batch_size = 4096\n",
    "ds_b = ds.map(break_into_sentences)\n",
    "ds_b = ds_b.map(custom_standardization)\n",
    "ds_b = ds_b.unbatch().map(add_markers).batch(batch_size)\n",
    "ds_b = ds_b.map(gen_bigrams)\n",
    "ds_b = ds_b.map(imdb_vectorization)\n",
    "ds_b = ds_b.map(lambda rt: tf.reshape(rt.flat_values, (-1,2)))\n",
    "ds_b = ds_b.unbatch().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "001b3388-ee34-4f27-b14c-eb6092d089de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse tensor to store the frequency of appearance of bigrams.\n",
    "bigram_freq = tf.sparse.SparseTensor(tf.constant([[]], dtype=tf.int64, shape=(0,2)),\n",
    "                                     tf.constant([], dtype=tf.int64),\n",
    "                                     dense_shape=(imdb_vocab_size,imdb_vocab_size))\n",
    "\n",
    "# Count the frequency of appearance of bigrams in the dataset\n",
    "for batch in ds_b:\n",
    "    x = tf.sparse.SparseTensor(batch,\n",
    "                               tf.ones(shape=(batch.shape[0]), dtype=tf.int64),\n",
    "                               dense_shape=(imdb_vocab_size,imdb_vocab_size))\n",
    "    bigram_freq = tf.sparse.add(bigram_freq, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54332c79-963c-44d3-9bf2-e69455819053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sparse tensor indices / Number of all bigrams =  12146027 / 14400000000 \n",
      "\n",
      "Maximum value recorded in sparse tensor =  19\n"
     ]
    }
   ],
   "source": [
    "# Stats on frequency of appearance of bigrams\n",
    "print('Number of sparse tensor indices / Number of all bigrams = ',\n",
    "      f'{bigram_freq.values.shape[0]} / {imdb_vocab_size**2} \\n')\n",
    "\n",
    "print('Maximum value recorded in sparse tensor = ', \n",
    "      tf.reduce_max(bigram_freq.values).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8e207-c8d4-4801-8b52-b55a2fabc338",
   "metadata": {},
   "source": [
    "#### **Caution**: Repeated indices in sparse tensors\n",
    "\n",
    "The above stats are obviously incorrect. The reason can be traced back to repeated indices. The way sparse tensors and sparse tensor operations are coded in tensorflow, it is possible to define or obtain sparse tensors with repeated indices:\n",
    "\n",
    "```\n",
    "x = tf.sparse.SparseTensor(tf.constant([[0,1], [1,1], [0,1]]),\n",
    "                           tf.constant([2, 4, 8]),\n",
    "                           dense_shape=(2,2))\n",
    "```\n",
    "However, if you attempt to convert this sparse tensor to a dense tensor with validate_indices=True you will get an error, and with validate_indices=False you will get the last value for each repeated index and **not the sum**.\n",
    "\n",
    "```\n",
    "tf.sparse.to_dense(x, validate_indices=False) = [[0, 8],[0, 4]]\n",
    "```\n",
    "You can do this operation in SciPy instead where by default the values for repeated indices are summed over, or explicitly sum over the values for repeated indices (not very efficient!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea4465-d0bf-4e32-98c2-2bd731606e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to sum over the values of repeated indices\n",
    "# for a 2 dimensional sparse tensor.\n",
    "def reduce_sum_rsi(x : tf.sparse.SparseTensor,\n",
    "                   batch_size = 512) -> tf.sparse.SparseTensor:\n",
    "    ''' Reduces a sparse 2 dimensional tensor with repeated \n",
    "    indices to a sparse tensor with valid (not repeated) indices \n",
    "    by summing over the (integer) values for the repeated indices.\n",
    "    This operation is done batch-wise by slicing unique(x.indices).\n",
    "    \n",
    "    Inputs:\n",
    "    x: A 2-dimensional sparse tensor with dtype = tf.int64\n",
    "    batch_size: A positive integer. \n",
    "    (Assumes batch_size < len(x.indices) is True.)\n",
    "\n",
    "    Returns an integer-valued tf.spare.SparseTensor with the\n",
    "    same dense_shape as x.\n",
    "    '''\n",
    "    # Select all unique index pairs\n",
    "    indices = tf.raw_ops.UniqueV2(x=x.indices, axis = [0])[0]\n",
    "    # Initialize the values tensor\n",
    "    values = tf.Variable(tf.zeros(shape = (indices.__len__(),),\n",
    "                                  dtype=tf.float32))\n",
    "    \n",
    "    # Batch parameters\n",
    "    num_indices = x.indices.__len__()\n",
    "    num_batches = num_indices // batch_size\n",
    "    if num_indices % batch_size:\n",
    "        num_batches +=1\n",
    "    \n",
    "    # Loop over batches\n",
    "    pbar = keras.utils.Progbar(target=num_batches-1)\n",
    "    s = batch_size\n",
    "    for k in range(num_batches):\n",
    "        # Adjust batch_size for the last batch\n",
    "        if k == num_batches-1 and num_indices % batch_size:\n",
    "            s = num_indices % batch_size\n",
    "        \n",
    "        batch = tf.slice(x.indices, [k*batch_size,0], [s, 2])\n",
    "        # Match x in batch with y in indices\n",
    "        # mask.shape = (batch_size, num_indices)\n",
    "        mask = tf.reduce_all(\n",
    "            tf.equal(tf.expand_dims(batch, axis=1), indices),\n",
    "            axis=2)\n",
    "        # Convert to sparse tensor with dtyep=float32\n",
    "        # (necessary for sparse_dense_matmul)\n",
    "        mask = tf.sparse.SparseTensor(\n",
    "            tf.where(mask),\n",
    "            tf.ones(shape=(tf.where(mask).__len__(),),\n",
    "                    dtype=tf.float32),\n",
    "            dense_shape = mask.shape)\n",
    "        # Update values\n",
    "        values.assign_add(\n",
    "            tf.reshape(\n",
    "                tf.sparse.sparse_dense_matmul(\n",
    "                    tf.expand_dims( tf.cast(\n",
    "                        tf.slice(bigram_freq.values,\n",
    "                                 [k*batch_size,], [s,]),\n",
    "                        dtype = tf.float32), axis = 0),\n",
    "                    mask), \n",
    "                shape = (-1,))\n",
    "        )\n",
    "        pbar.update(k)\n",
    "\n",
    "    # Convert to dtype=int64\n",
    "    values = tf.cast(values, tf.int64)\n",
    "    \n",
    "    return tf.sparse.SparseTensor(indices, values, x.dense_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afadb6-dc36-4267-854d-353014c03967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over the values of repeated indices in bigram_freq\n",
    "bigram_freq_validated = reduce_sum_rsi(bigram_freq, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9135694d-8e51-4b65-98db-07a394f62c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique positive bigrams / Number of all bigrams =  2253110 / 14400000000\n"
     ]
    }
   ],
   "source": [
    "# Stats on frequency of appearance of bigrams\n",
    "print('Number of unique positive bigrams / Number of all bigrams = ',\n",
    "      '{} / {}'.format(\n",
    "          tf.raw_ops.UniqueV2(\n",
    "              x=bigram_freq.indices, axis = [0])[0].__len__(), \n",
    "          imdb_vocab_size**2)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70e4ce-126c-4aad-81cb-e02585a12864",
   "metadata": {},
   "source": [
    "For large num_unique_indices we also need to loop over unique indices, below is a modified version of reduced_sum_rsi for this case.\n",
    "\n",
    "```\n",
    "def reduce_sum_rsi_v2(x : tf.sparse.SparseTensor,\n",
    "                   batch_size = 512) -> tf.sparse.SparseTensor:\n",
    "    ''' Reduces a sparse 2 dimensional tensor with repeated \n",
    "    indices to a sparse tensor with valid (not repeated) indices \n",
    "    by summing over the (integer) values for the repeated indices.\n",
    "    This operation is done batch-wise by slicing unique(x.indices).\n",
    "    \n",
    "    Inputs:\n",
    "    x: A 2-dimensional sparse tensor with dtype = tf.int64\n",
    "    batch_size: A positive integer. \n",
    "    (Assumes batch_size < len(x.indices) is True.)\n",
    "\n",
    "    Returns an integer-valued tf.spare.SparseTensor with the\n",
    "    same dense_shape as x.\n",
    "    '''\n",
    "    # Select all unique index pairs\n",
    "    indices = tf.raw_ops.UniqueV2(x=x.indices, axis = [0])[0]\n",
    "    # Initialize the values tensor\n",
    "    values = tf.Variable(tf.zeros(shape = (indices.__len__(),), dtype=tf.int64))\n",
    "    \n",
    "    # Batch parameters\n",
    "    s1 = batch_size\n",
    "    num_indices = x.indices.__len__()\n",
    "    num_batches = num_indices // batch_size\n",
    "    if num_indices % batch_size:\n",
    "        num_batches +=1\n",
    "\n",
    "    num_unique_indices = indices.__len__()\n",
    "    num_unique_batches = num_unique_indices // batch_size\n",
    "    if num_unique_indices % batch_size:\n",
    "        num_unique_batches +=1\n",
    "        \n",
    "    # Loop over batches\n",
    "    pbar = keras.utils.Progbar(target=num_batches-1)\n",
    "    for k in range(num_batches):\n",
    "        # Adjust batch_size for the last batch\n",
    "        if k == num_batches-1 and num_indices % batch_size:\n",
    "            s1 = num_indices % batch_size\n",
    "        \n",
    "        batch = tf.slice(x.indices, [k*batch_size,0], [s1, 2])\n",
    "        s2 = batch_size\n",
    "        for j in range(num_unique_batches):\n",
    "            if j == num_unique_batches-1 and num_unique_indices % batch_size:\n",
    "                s2 = num_unique_indices % batch_size\n",
    "            # Match x in batch with y in indices\n",
    "            # mask.shape = (s1, s2)\n",
    "            mask = tf.reduce_all(\n",
    "                tf.equal(tf.expand_dims(batch, axis=1), \n",
    "                         tf.slice(indices, [j*batch_size,0], [s2, 2])),\n",
    "                axis=2)\n",
    "            # Convert to dtyep=int64\n",
    "            mask = tf.cast(mask, dtype=tf.int64)\n",
    "            # Update values\n",
    "            values[j*batch_size:j*batch_size+s2].assign_add(\n",
    "                tf.reshape(\n",
    "                    tf.matmul(\n",
    "                        tf.expand_dims(\n",
    "                            tf.slice(bigram_freq.values,[k*batch_size,], [s1,]),\n",
    "                            axis = 0),\n",
    "                        mask),\n",
    "                    shape = (-1,))\n",
    "            )\n",
    "        \n",
    "        pbar.update(k)\n",
    "    \n",
    "    return tf.sparse.SparseTensor(indices, values, x.dense_shape)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pip]",
   "language": "python",
   "name": "conda-env-pip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
