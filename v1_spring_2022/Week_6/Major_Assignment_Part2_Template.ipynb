{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Major_Assignment_Part2_Template.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L4erH6yTf0L0",
        "oyZcLtEuQ7bm",
        "VpmrayBJRqKk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Student Information:\n",
        "- Name:\n",
        "- BU email:\n",
        "- Collaborators:"
      ],
      "metadata": {
        "id": "10sGyp36wzBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Major Assignment Part2\n",
        "\n",
        "You have already built and trained a model capable of recognizing a single digit from a 1s recording. The next step for our automated phone payment system is to extend the model to recognize 16 digits in a row (I made a mistake earlier saying that there are 12 digits!). Here is what you need to do:\n",
        "\n",
        "1.   Extend the single-digit voice recognition model to take a 16*16000 component waveform and output 16 digits. For simplicity you can assume that each second of the input contains the recording of a single digit.\n",
        "\n",
        "\n",
        "2.   *Optional*: Provide a method to convert a recording of someone saying 16 digits in a row to a 16*16000 component verctor, A,  where \n",
        "A[16000 j,16000 (j+1)] \n",
        "contains a recording of a single digit.\n",
        "\n",
        "Be sure to submit this notebook as well as the saved weights of your final model in the h5 format."
      ],
      "metadata": {
        "id": "iKx-k3HifUSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up - DO NOT EDIT THIS SECTION"
      ],
      "metadata": {
        "id": "L4erH6yTf0L0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_dp4FO5wX0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193c3886-08a8-41d8-b653-c1cfe0cdfb17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.4 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.25.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.25.0)\n",
            "Installing collected packages: tensorflow-io\n",
            "Successfully installed tensorflow-io-0.25.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "uRnR28LYwmzT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_links = {'train_data': 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n",
        "                 'test_data': 'http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz'}"
      ],
      "metadata": {
        "id": "ibKxWgX1wt2B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in dataset_links:\n",
        "    tf.keras.utils.get_file(key+'.tar.gz',\n",
        "                            dataset_links[key],\n",
        "                            cache_dir='./',\n",
        "                            cache_subdir='datasets/'+key,\n",
        "                            extract=True)"
      ],
      "metadata": {
        "id": "JTpJ6wWTwuYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26819d4a-d0ba-4f93-95b7-767a49531c04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "2428928000/2428923189 [==============================] - 50s 0us/step\n",
            "2428936192/2428923189 [==============================] - 50s 0us/step\n",
            "Downloading data from http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz\n",
            "112566272/112563277 [==============================] - 4s 0us/step\n",
            "112574464/112563277 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_paths = []\n",
        "for folder, labels, samples in os.walk('./datasets/train_data/'):\n",
        "    for sample in samples:\n",
        "        if sample[-3:] == 'wav':\n",
        "            train_data_paths.append([folder+'/'+sample, folder[22:]])\n",
        "\n",
        "df = pd.DataFrame(train_data_paths, columns=['paths', 'labels'])\n",
        "df = df.drop(df[df['labels'] =='_background_noise_'].index)\n",
        "categories = df['labels'].unique()\n",
        "digits_dict = {'zero':0, 'one':1, 'two':2, \n",
        "               'three':3, 'four':4, 'five':5,\n",
        "               'six':6, 'seven':7, 'eight':8,\n",
        "               'nine':9}\n",
        "digits_index = []\n",
        "for digit in digits_dict.keys():\n",
        "    digits_index = digits_index + list(df[df['labels']==digit].index)\n",
        "df = df.loc[digits_index]\n",
        "df = df.sample(frac=1)\n",
        "df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "Z_-INzgUwxYC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio import function with padding\n",
        "def load_audio(filepath):\n",
        "    \"\"\"Takes the path of a wav audio file as input and creates\n",
        "    a numpy array of shape (16000) as output. The input file\n",
        "    needs to sample rate=16000. The expected duration is 1s,\n",
        "    shorter samples are padded at the end while longer samples\n",
        "    are cropped at 1s.\"\"\"\n",
        "    audio = tfio.audio.AudioIOTensor(filepath)\n",
        "    audio_rate = int(audio.rate)\n",
        "    assert audio_rate == 16000\n",
        "    audio = audio.to_tensor().numpy().reshape((-1)) / 32767.0\n",
        "    audio = audio.astype(dtype=\"float32\")\n",
        "    len = audio.shape[0]\n",
        "    # Padding\n",
        "    if len == 16000:\n",
        "        return audio\n",
        "    elif len < 16000:\n",
        "        return np.concatenate([audio, \n",
        "                               np.zeros(shape=(16000-len),\n",
        "                                        dtype=\"float32\")], \n",
        "                              axis=0)\n",
        "    else:\n",
        "        return audio[0:16000]\n",
        "\n",
        "\n",
        "# The dataset class used to feed data to our model during training and evaluation.\n",
        "class audio_gen(keras.utils.Sequence):\n",
        "    def __init__(self, file_paths, labels,\n",
        "                 batch_size=32, shape=(16*16000,),\n",
        "                 shuffle_on_epoch_end=True):\n",
        "        # Initialization\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.paths = file_paths\n",
        "        self.n_channels = 1\n",
        "        self.n_classes = 10\n",
        "        self.shuffle = shuffle_on_epoch_end\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.paths) / self.batch_size))\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        batch_paths = self.paths[self.batch_size * idx: \n",
        "                                 self.batch_size * (idx+1)]\n",
        "        batch_labels = self.labels[self.batch_size * idx:\n",
        "                                   self.batch_size * (idx+1)]\n",
        "        batch_samples = np.zeros(shape=(0, self.shape[0]), \n",
        "                                 dtype='float32')\n",
        "        for paths in batch_paths:\n",
        "            sample = np.zeros(shape=(0), dtype='float32')\n",
        "            for path in paths:\n",
        "                sample = np.concatenate([sample, load_audio(path)], axis=0)\n",
        "            batch_samples = np.concatenate([batch_samples, [sample]], axis=0)\n",
        "        return batch_samples, np.array(batch_labels, dtype='int')\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # suffle the dataset after each epoch\n",
        "        if self.shuffle:\n",
        "            self.paths, self.labels = shuffle(self.paths, self.labels)"
      ],
      "metadata": {
        "id": "E9ysvj5s37--"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will ignore the constraints on credit card numbers for now.\n",
        "\n",
        "train_paths = np.array(df['paths'])[:32000].reshape((-1,16))\n",
        "train_labels = np.array([digits_dict[x] for x in df['labels']])[0:32000].reshape((-1,16))\n",
        "\n",
        "valid_paths = np.array(df['paths'])[32000:35200].reshape((-1,16))\n",
        "valid_labels = np.array([digits_dict[x] for x in df['labels']])[32000:35200].reshape((-1,16))\n",
        "\n",
        "test_paths = np.array(df['paths'])[35200:38896].reshape((-1,16))\n",
        "test_labels = np.array([digits_dict[x] for x in df['labels']])[35200:38896].reshape((-1,16))"
      ],
      "metadata": {
        "id": "YhYIKrff6M7B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = audio_gen(train_paths, train_labels)\n",
        "valid_gen = audio_gen(valid_paths, valid_labels)\n",
        "test_gen = audio_gen(test_paths, test_labels)"
      ],
      "metadata": {
        "id": "8pj6kvjj8MLT"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spectrogram(audio_tensor):\n",
        "    return tfio.audio.spectrogram(audio_tensor,\n",
        "                                  nfft=512,\n",
        "                                  window=256,\n",
        "                                  stride=128)\n",
        "\n",
        "def mel_spectrogram(audio_tensor):\n",
        "    return tfio.audio.melscale(get_spectrogram(audio_tensor),\n",
        "                               rate=16000,\n",
        "                               mels=128,\n",
        "                               fmin=0,\n",
        "                               fmax=8000)\n",
        "\n",
        "\n",
        "def dbscale_spectrogram(audio_tensor):\n",
        "    return tfio.audio.dbscale(mel_spectrogram(audio_tensor),\n",
        "                              top_db=80)/60.0"
      ],
      "metadata": {
        "id": "dXhhLa_WWQf_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metric"
      ],
      "metadata": {
        "id": "oyZcLtEuQ7bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model's prediction is accurate only if the model predicts all 16\n",
        "# digits correctly. The custom metric below can be used to evaluate the\n",
        "# performance of the model.\n",
        "\n",
        "class seq_accuracy(keras.metrics.Metric):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(seq_accuracy, self).__init__()\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Takes the product of single-digit accuracies for each 16-digit sample.\n",
        "        accuracies = tf.reduce_prod(\n",
        "            tf.cast(tf.equal(y_true, tf.argmax(y_pred, axis=2)), \n",
        "                    tf.float32), axis=1)\n",
        "        sum_a = tf.reduce_sum(accuracies)\n",
        "        with tf.control_dependencies([sum_a]):\n",
        "            update_t = self.total.assign_add(sum_a)\n",
        "        num_a = tf.cast(tf.size(accuracies), self._dtype)\n",
        "        with tf.control_dependencies([update_t]):\n",
        "            return self.count.assign_add(num_a)\n",
        "    \n",
        "    def result(self):\n",
        "        return tf.math.divide_no_nan(self.total, self.count)\n",
        "    \n",
        "    def reset_states(self):\n",
        "        self.total.assign(0.)"
      ],
      "metadata": {
        "id": "_9OrwM9uMPNy"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample inputs and outputs"
      ],
      "metadata": {
        "id": "VpmrayBJRqKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample inputs and labels in our dataset:\n",
        "for batch in train_gen:\n",
        "    sample_inputs = batch[0]\n",
        "    print(sample_input.shape)\n",
        "    sample_labels = batch[1]\n",
        "    print(sample_labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "wR8T7qkYoKOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1205e7-0c3c-448d-dabd-7f68830d8333"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 256000)\n",
            "(16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample outputs:\n",
        "# predictions = model_16.predict(sample_inputs)\n",
        "# predictions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmFURdQMlMDq",
        "outputId": "8972e305-3f3b-40dc-8aa5-ad0f97fc3612"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 16, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference:\n",
        "np.argmax(predictions, axis=2)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTywBLbaPYcN",
        "outputId": "aecc8d31-1d29-4d04-fe69-140a29802576"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 5, 6, 3, 8, 1, 5, 0, 6, 8, 3, 5, 6, 4, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uuporPpPpQm",
        "outputId": "054993bb-9558-4801-b4bb-09a79ea666af"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 5, 6, 3, 8, 1, 5, 0, 6, 8, 3, 5, 6, 4, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Edit the cells below"
      ],
      "metadata": {
        "id": "rvRx8cROtR2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import your single-digit voice recognition model\n",
        "\n",
        "Import your pretrained single-digit classifier. The model should take 16000-component 'waveform' vectors as input and produce a 10-component 'softmax' vector."
      ],
      "metadata": {
        "id": "Zx7CJ16PjtdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-digit classifier structure:\n",
        "#\n",
        "# model_1_inputs = keras.Input(shape=(16000,))\n",
        "# x = layers.Dense(10, activation='softmax')(model_1_inputs)\n",
        "#\n",
        "# model_1 = keras.Model(inputs = model_1_inputs, outputs = x)\n",
        "# model_1.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "#\n",
        "# model_1.load_weights('./model_1.h5')"
      ],
      "metadata": {
        "id": "R7AURqCvkLpG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extend to 16 digits\n",
        "\n",
        "Extend your single-digit classifier to classify 16 digits in parallel. The expected input of the model is a 256000-component 'waveform' while the output should be a (16,10)-shaped tensor or sixteen 10-component vectors where each 10-component vector corresponds to the classification of a single digit.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Hint: Consider the toy-classifier below which classifies a single digit recording.\n",
        "\n",
        "```\n",
        "inputs = keras.Input(shape=(16000,))\n",
        "x = layers.Dense(400, activation = 'relu')(inputs)\n",
        "x = layers.Dense(10, activation = 'softmax')(x)\n",
        "classifier1 = keras.Model(inputs = inputs, outputs = x)\n",
        "```\n",
        "\n",
        "To extend this to 16 parallel classifiers we can play a simple trick inspired by the object detection model we discussed in class:\n",
        "\n",
        "```\n",
        "inputs = keras.Input(shape=(16*16000,))\n",
        "x = layers.Reshape((16,16000))(inputs)\n",
        "x = layers.Dense(400, activation = 'relu')(x)\n",
        "x = layers.Dense(10, activation = 'softmax')(x)\n",
        "classifier16 = keras.Model(inputs = inputs, outputs = x)\n",
        "```\n",
        "This is equivalent to running 16 single-digit classifiers in parallel and feeding each only a portion of the input (input[16000 j: 16000(j+1)]).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ccn87TbtkF6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your extended model"
      ],
      "metadata": {
        "id": "VLuAaKocRDZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16-digit classifier structure:\n",
        "#\n",
        "# model_16_inputs = keras.Input(shape=(16*16000,))\n",
        "# x = layers.Reshape((16, 16000))(model_16_inputs)\n",
        "# x = layers.Dense(10, activation='softmax')(x)\n",
        "#\n",
        "# model_16 = keras.Model(inputs = model_16_inputs, outputs = x)\n",
        "# model_16.compile(optimizer=\"adam\", loss=loss, metrics = [metric])\n",
        "#"
      ],
      "metadata": {
        "id": "_o04kuSnlqkD"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the custom metric seq_accuracy for evaluating the performance of your model.\n",
        "metric = seq_accuracy()\n",
        "loss = keras.losses.sparse_categorical_crossentropy\n",
        "model_16.compile(optimizer=\"adam\", loss=loss, metrics=[metric])\n",
        "model_16.evaluate(test_gen)"
      ],
      "metadata": {
        "id": "l_SskyIXutoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iEyXbu5FZGpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing (*Optional*)\n",
        "\n",
        "So far we assumed that the waveform vector was parsed such that each 16000-long segment records a single digit. This may not be the case for a real recording thus we need to preprocess the input. If you feel extra motivated you can try writing a function which implements the following:\n",
        "\n",
        "\n",
        "1.   Take as an input a waveform of arbitrary length\n",
        "2.   Locate the spoken digits in the waveform and check that there are 16 of them.\n",
        "3.   Pad/crop the waveform such that the spoken digits are located in 1s long segments.\n",
        "4.   Return the resulting 256000 component vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "siqDx0QOj3Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dpnz1W7MUh-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}