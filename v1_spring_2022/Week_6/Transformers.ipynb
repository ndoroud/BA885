{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq_EngFr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence to sequence models"
      ],
      "metadata": {
        "id": "UjpWClSFsrUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "AXKc-Y7Js6LO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "mxSBTPKZuf3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a language dataset provided by http://www.manythings.org/anki/ this time containing English and French sentences."
      ],
      "metadata": {
        "id": "PQ68WF6VvGsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.get_file('fra-eng.zip',\n",
        "                        'http://www.manythings.org/anki/fra-eng.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ],
      "metadata": {
        "id": "J79Nm3Olsu5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU_UlF7rEKmA",
        "outputId": "5291d659-7ee5-4ab2-f394-8a06e3a35bdb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 6379k  100 6379k    0     0  33.8M      0 --:--:-- --:--:-- --:--:-- 33.8M\n",
            "Archive:  fra-eng.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: _about.txt              \n",
            "replace fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the \"fra.txt\" text file which contains pairs of english & french sentences. \n",
        "text_file = \"fra.txt\"\n",
        "# Split on new lines to get a single pair.\n",
        "with open(text_file, \"r\", encoding=\"utf-8\") as text:\n",
        "    lines = text.read().split(\"\\n\")\n",
        "text_pairs = []\n",
        "# For each pair, split the english sentence from the french one using the \"\\t\" marker.\n",
        "# Mark the begining and the end of the french translation with \"[start]\" and \"[eng]\".\n",
        "for line in lines:\n",
        "    english = line.split(\"\\t\")[0]\n",
        "    if len(line.split(\"\\t\"))>1:\n",
        "        french = line.split(\"\\t\")[1]\n",
        "        french = \"[start] \" + french + \" [end]\"\n",
        "        text_pairs.append((english, french))"
      ],
      "metadata": {
        "id": "ixz3rwLFteuS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRAmEuLsEFmL",
        "outputId": "8d408764-6923-40f5-bd79-3a9963b30ac0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192341"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbPHhPOIt7rr",
        "outputId": "48f12318-c19b-4c71-de6d-3fe17b742ef6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Go.', '[start] Va ! [end]'),\n",
              " ('Go.', '[start] Marche. [end]'),\n",
              " ('Go.', '[start] Bouge ! [end]'),\n",
              " ('Hi.', '[start] Salut ! [end]'),\n",
              " ('Hi.', '[start] Salut. [end]'),\n",
              " ('Run!', '[start] Cours\\u202f! [end]'),\n",
              " ('Run!', '[start] Courez\\u202f! [end]'),\n",
              " ('Run!', '[start] Prenez vos jambes à vos cous ! [end]'),\n",
              " ('Run!', '[start] File ! [end]'),\n",
              " ('Run!', '[start] Filez ! [end]')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the text pairs before splitting them into training, validation and test sets.\n",
        "text_pairs = shuffle(text_pairs)\n",
        "\n",
        "num_train = int(0.7 * len(text_pairs))\n",
        "num_valid = int(0.2 * len(text_pairs))\n",
        "\n",
        "train_pairs = text_pairs[:num_train]\n",
        "valid_pairs = text_pairs[num_train:num_train + num_valid]\n",
        "test_pairs = text_pairs[num_train + num_valid:]"
      ],
      "metadata": {
        "id": "MrkJWDGHxO1j"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F07797CuC4yz",
        "outputId": "f34e3eef-e97d-4e2a-edfd-8a6125601a1e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('He has dedicated his life to the preservation of nature.',\n",
              "  \"[start] Il a dédié sa vie à la préservation de l'environnement. [end]\"),\n",
              " (\"It's you who've acted inappropriately.\",\n",
              "  \"[start] C'est vous qui avez agi de manière inappropriée. [end]\"),\n",
              " ('How could you do that?', '[start] Comment as-tu pu faire ça ? [end]'),\n",
              " ('The more laws, the more offenders.',\n",
              "  '[start] Plus il y a de lois, plus il y a de délinquants. [end]'),\n",
              " (\"I'm going to Australia to visit my family for Christmas.\",\n",
              "  '[start] Je vais aller en Australie pour rendre visite à ma famille pour Noël. [end]'),\n",
              " ('I suggest that you not wait any longer.',\n",
              "  \"[start] Je suggère que tu n'attendes pas davantage. [end]\"),\n",
              " ('She likes to arrange flowers.',\n",
              "  '[start] Elle aime composer des arrangements floraux. [end]'),\n",
              " ('We were outnumbered.', '[start] Nous avons été surpassés en nombre. [end]'),\n",
              " ('We enjoyed watching the fireworks on a bridge last summer.',\n",
              "  \"[start] Nous avons apprécié regarder le feu d'artifice sur un pont l'été dernier. [end]\"),\n",
              " ('He read the poem with a loud voice.',\n",
              "  '[start] Il lut le poème à voix haute. [end]')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "I9ElURlDv_84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardization & vectorization"
      ],
      "metadata": {
        "id": "3AC2hUIwwGsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UUAs6V10Erf_",
        "outputId": "2066cf15-d992-41a3-bd5d-ce2d4bbc53a0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import punctuations and add \"\\u202f\" for french text.\n",
        "strip_chars = string.punctuation + \"\\u202f\"\n",
        "# Remove square brackets from puntuations since we use them to mark\n",
        "# the endpoints of french sentences.\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    # Standardizes a string by convering it to all lowercase\n",
        "    # and removing punctuations in the strip_chars string.\n",
        "    # Returns a tf.string.\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# Restrict the size of the vocabulary and the sequence length.\n",
        "# The least frequent words beyond vocab_size will be classified as [UNK].\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "# TextVectorization layer for english sentences\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# TextVectorization layer for french sentences\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    # target sentences are longer since they start with [start].\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# Adapt the vectorization layers to the corresponding sentences.\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_french_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_french_texts)"
      ],
      "metadata": {
        "id": "kToH53RQuEK5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_english_texts[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km2Fn2OHHFXB",
        "outputId": "2c5c3c7f-83b3-457a-d58a-04a15b3b5c64"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He has dedicated his life to the preservation of nature.',\n",
              " \"It's you who've acted inappropriately.\",\n",
              " 'How could you do that?',\n",
              " 'The more laws, the more offenders.',\n",
              " \"I'm going to Australia to visit my family for Christmas.\",\n",
              " 'I suggest that you not wait any longer.',\n",
              " 'She likes to arrange flowers.',\n",
              " 'We were outnumbered.',\n",
              " 'We enjoyed watching the fireworks on a bridge last summer.',\n",
              " 'He read the poem with a loud voice.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Indices 0 and 1 are always reserved for '' and '[UNK]'.\n",
        "source_vectorization.get_vocabulary()[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyl_JYNiHpk2",
        "outputId": "344986a8-e770-4766-ef7d-fc3382826bf1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'i', 'you', 'to', 'the', 'a', 'is', 'tom', 'that']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_vectorization.vocabulary_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2OHxlgHNAJ4",
        "outputId": "4e6ca9e9-24fe-4340-f37a-88d6d4ad558d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14141"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The french sentences contain more that 15000 unique words.\n",
        "target_vectorization.vocabulary_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkYC38SuHeaG",
        "outputId": "46b82cc6-7c71-4746-8b74-e679951f0c7a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build datasets from our training and validation sets. The important object\n",
        "# here is \"tf.data.Dataset.from_tensor_slices\" which is a fancier version of\n",
        "# the generators we have encountered before.\n",
        "batch_size = 64\n",
        "\n",
        "# Note that the input is a dictionary with both the english and the french\n",
        "# version of the sentence. The enlish sentence is the input of the encoder\n",
        "# and the french sentence is the input of the decoder during training.\n",
        "\n",
        "def format_dataset(eng, fre):\n",
        "    # Passes eng and fre through source and target vectorization layers \n",
        "    # respectively. Returns a tuple (dict, v_fre) where dict is a dictionary and\n",
        "    # v_fre is vectorized french sentence without markers ([start],[end]).\n",
        "    eng = source_vectorization(eng)\n",
        "    fre = target_vectorization(fre)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"french\": fre[:, :-1],\n",
        "    }, fre[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    # Create a tf.dataset from our english-french pairs of sentences.\n",
        "    eng_texts, fre_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    fre_texts = list(fre_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fre_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "valid_ds = make_dataset(valid_pairs)"
      ],
      "metadata": {
        "id": "sn3OmUCJzXBg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is what the output of format_dataset looks like:\n",
        "format_dataset([train_pairs[0][0]], [train_pairs[0][1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oLOocnFPBH9",
        "outputId": "7d0d927c-336c-41b0-8555-d304db49040f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'english': <tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
              "  array([[   12,    67,  3056,    41,   236,     4,     5, 10868,    11,\n",
              "           1893,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0]])>,\n",
              "  'french': <tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
              "  array([[   2,   14,   18, 8120,   88,  194,    9,   10,    1,    5, 6277,\n",
              "             3,    0,    0,    0,    0,    0,    0,    0,    0]])>},\n",
              " <tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
              " array([[  14,   18, 8120,   88,  194,    9,   10,    1,    5, 6277,    3,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0]])>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset generates tuples -- created via format_dataset -- of batches of \n",
        "# size batch_size (=64).\n",
        "for x in train_ds:\n",
        "    print(x)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-041GSKUdvq",
        "outputId": "0f6e7b0c-cee4-42c4-875f-0245f54e2fd6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'english': <tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n",
            "array([[   2,   31,   24, ...,    0,    0,    0],\n",
            "       [   2,  256,   24, ...,    0,    0,    0],\n",
            "       [1142,    3,  290, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,   19,  377, ...,    0,    0,    0],\n",
            "       [  57,   64,  174, ...,    0,    0,    0],\n",
            "       [  53,    3,  231, ...,    0,    0,    0]])>, 'french': <tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n",
            "array([[   2,    4,  375, ...,    0,    0,    0],\n",
            "       [   2,    4,   27, ...,    0,    0,    0],\n",
            "       [   2,    5,  105, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,   24,  201, ...,    0,    0,    0],\n",
            "       [   2,   89,    8, ...,    0,    0,    0],\n",
            "       [   2,   19, 3786, ...,    0,    0,    0]])>}, <tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n",
            "array([[   4,  375,   78, ...,    0,    0,    0],\n",
            "       [   4,   27,  684, ...,    0,    0,    0],\n",
            "       [   5,  105, 2421, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [  24,  201,  695, ...,    0,    0,    0],\n",
            "       [  89,    8,  368, ...,    0,    0,    0],\n",
            "       [  19, 3786,  696, ...,    0,    0,    0]])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['french'].shape: {inputs['french'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL9Zrrew0GwL",
        "outputId": "2a565c95-3fc5-475b-aa26-6680ff911006"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['french'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence to sequence model"
      ],
      "metadata": {
        "id": "AmJhsJWs0OZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The seq2seq model is built by combining 2 RNNs: the encoder and the decoder. \n",
        "\n",
        "The purpose of the encoder is feature extraction: the output should be a sequence that encodes the meaning of the input sentence in a way that is (somewhat) independent of the language.\n",
        "\n",
        "The purpose of the decoder is to do the reverse, that is to build a sentence in french from the feature representation of the english sentence constructed by the encoder."
      ],
      "metadata": {
        "id": "HTJKDAvmZXwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "daC7dk1e0SqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The encoder has a simple structure: it takes a vectorized english sentence as\n",
        "# a sequence, embeds it a embed_dim-dimensional vector space and passes the\n",
        "# resulting sequence through a bidirectional GRU layer.\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "encoder_input = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(encoder_input)\n",
        "encoder_output = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "CC9DeSvF0HC_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "rGIFJb290y3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The decoder is treated differently during training and inference.\n",
        "# During training it takes the output of the encoder as well as the vectorized\n",
        "# french sentence as inputs. The french sentence is similarly passed through\n",
        "# an embedding layer.\n",
        "decoder_input = keras.Input(shape=(None,), dtype=\"int64\", name=\"french\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(decoder_input)\n",
        "\n",
        "# The decoder recurrent layer takes the embedded french sentence as its input\n",
        "# and the the encoder output as its initial state.\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoder_output)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# At each pass, the decoder's output is used to predict the next word/token.\n",
        "next_target = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "seq2seq_rnn = keras.Model(inputs = [encoder_input, decoder_input], \n",
        "                          outputs = next_target)"
      ],
      "metadata": {
        "id": "bbDtEonc00Je"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7oentjZCBkK",
        "outputId": "ef3b5141-51f6-4d0c-d282-eba389ddfe2d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " english (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " french (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    3840000     ['english[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    3840000     ['french[0][0]']                 \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 1024)         7876608     ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " gru_1 (GRU)                    (None, None, 1024)   3938304     ['embedding_1[0][0]',            \n",
            "                                                                  'bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, None, 1024)   0           ['gru_1[0][0]']                  \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 15000)  15375000    ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 34,869,912\n",
            "Trainable params: 26,993,304\n",
            "Non-trainable params: 7,876,608\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_pred = seq2seq_rnn.predict(train_ds.take(1))[0]"
      ],
      "metadata": {
        "id": "Pi8_QKA8l2dm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model outputs a sequence of predictions where each prediction\n",
        "# is a classification in our vocabulary of 15000 words.\n",
        "sample_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s7vFwtfmWqy",
        "outputId": "8584543e-eff9-4d9f-aa22-4125602611af"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 15000)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall what the target looks like:\n",
        "for x in train_ds:\n",
        "    print(x[1][0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-QJMvgCnQv4",
        "outputId": "2d2c5500-9d2b-421b-ce15-7f11f69046a2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  30 2548   31  406    3    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "OlNknUiZ1fkm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.load_weights('seq2seq_rnn.h5')"
      ],
      "metadata": {
        "id": "mrBt9k6pu_yA"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.layers[4].trainable=False"
      ],
      "metadata": {
        "id": "R6SZ7QNpOXne"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.fit(train_ds, epochs=1, validation_data=valid_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUg3fvG6OsFe",
        "outputId": "69d251fb-dbbf-4d25-8360-ff43e8c98817"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2104/2104 [==============================] - 14674s 7s/step - loss: 1.1915 - accuracy: 0.5406 - val_loss: 0.7239 - val_accuracy: 0.6646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83de448290>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "tIt2IYDvTjdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To de-vectorize the output of our seq2seq model we need the following.\n",
        "fre_vocab = target_vectorization.get_vocabulary()\n",
        "fre_index_lookup = dict(zip(range(len(fre_vocab)), fre_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    # Passes the english sentence input_sentence through the seq2seq_rnn model.\n",
        "    # Outputs the top prediction for the french translation.\n",
        "    #\n",
        "    # Vectorize the input sentence before passing it to the model.\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # Vectorize the partial output sentence.\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        # Run model.predict to predict the next word of the output sentence\n",
        "        # from the input sentence and the partial output sentence.\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        # Find the index of the word with the highest confidence value.\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        # Pick the corresponding word\n",
        "        sampled_token = fre_index_lookup[sampled_token_index]\n",
        "        # Add the word with the highest confidence value to the output sentence.\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        # Check if the model predicts that the sentence should end.\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "OJUOWZmL7aUx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on a few sentences for comparison.\n",
        "for _ in range(5):\n",
        "    ind = random.choice(range(len(test_pairs)))\n",
        "    input_sentence = test_pairs[ind][0]\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print('prediction: {}'.format(decode_sequence(input_sentence)))\n",
        "    print('target: {}'.format(test_pairs[ind][1]))"
      ],
      "metadata": {
        "id": "W1CmLUXH8GG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2302e9-9121-4ed5-93cb-44c1004f38e1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "He never touched wine.\n",
            "prediction: [start] il ne jamais jamais vin [end]\n",
            "target: [start] Il ne buvait jamais de vin. [end]\n",
            "-\n",
            "Do you want some cherry pie?\n",
            "prediction: [start] voulezvous que je ne vous [UNK] [end]\n",
            "target: [start] Veux-tu de la tarte aux cerises ? [end]\n",
            "-\n",
            "I will not tolerate this.\n",
            "prediction: [start] je ne vais pas [UNK] [end]\n",
            "target: [start] Je ne le tolérerai pas. [end]\n",
            "-\n",
            "Tom is my boss at work.\n",
            "prediction: [start] tom est le meilleur au travail [end]\n",
            "target: [start] Tom est mon supérieur hiérarchique. [end]\n",
            "-\n",
            "I can't put up with the way he spits.\n",
            "prediction: [start] je ne peux pas supporter le chemin [end]\n",
            "target: [start] Je ne peux pas supporter sa manière de cracher. [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "nVisd7LRWtZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RNNs to build a machine translation network has quite a few shortcommings, the most apparent being:\n",
        "- Our text embeddings so far fail to capture context dependence of words/tokens.\n",
        "- While RNNs do capture word ordering, they are also designed to respect the ordering. However the ordering of words is not so strict and can be completely different in different languages.\n",
        "\n",
        "It would seem like a better idea then to focus instead on capturing the context. This is the idea behind self attention."
      ],
      "metadata": {
        "id": "IvcImEi2XJxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding"
      ],
      "metadata": {
        "id": "mO8EHFc9a5O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Switching from RNNs to Transformers, we trade word order awareness for context awareness. But we can always add the ordering of the words as an extra input!"
      ],
      "metadata": {
        "id": "JLXi3RuEa-Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    # Combines the standard embedding layer with a position embedding.\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # The standard embedding layer embedding the vectorized tokens.\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        # Position embedding layer embedding the sequence index.\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "1dSwPig0WxiB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Encoder"
      ],
      "metadata": {
        "id": "chtt__gnTVwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer encoder combines multi-head attention mechanism together with dense projections."
      ],
      "metadata": {
        "id": "wT0cvysic-BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Dimension of the vector space where tokens are represented as dense vectors.\n",
        "        self.embed_dim = embed_dim\n",
        "        # Dimension of the projected or \"downsampled\" dense vectors.\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # The projection module downsamples and upsamples the inputs.\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            # The mask will be generated by the preceding embedding layer and\n",
        "            # needs to be reshaped to the correct shape.\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        # Add a residual connection after the attention layer.\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        # Add a residual connection after the dense projection module.\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "gaL7XpK7TU-s"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Decoder"
      ],
      "metadata": {
        "id": "zN-bS8kJjMUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The crutial difference between the decoder and the encoder is that it has 2 inputs: \n",
        "- the english sentence passed through the encoder\n",
        "- and the (partial) french sentence\n",
        "\n",
        "The partial french sentence should be treated the same way as the english one so it needs to pass through a PositionalEmbedding layer as well as a MultiHeadAttention layer."
      ],
      "metadata": {
        "id": "JXCmfdICkk6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        # The attention layer for the partial target input.\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # The attention layer that compares the two inputs.\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # Dense projection following the attention layers.\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            # Mask for the \"upper-half\" of the partial target input.\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "N2_48HkDjLzC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-end Transformer"
      ],
      "metadata": {
        "id": "yIWSPjNxprEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"french\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "TBk1RMZWpvut"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odV8t2udqvcq",
        "outputId": "0e9cd443-32be-433d-e0d3-b27715ebdafa"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " english (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " french (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding_2 (Positi  (None, None, 256)   3845120     ['english[0][0]']                \n",
            " onalEmbedding)                                                                                   \n",
            "                                                                                                  \n",
            " positional_embedding_3 (Positi  (None, None, 256)   3845120     ['french[0][0]']                 \n",
            " onalEmbedding)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Transfo  (None, None, 256)   2170496     ['positional_embedding_2[0][0]'] \n",
            " rmerEncoder)                                                                                     \n",
            "                                                                                                  \n",
            " transformer_decoder_1 (Transfo  (None, None, 256)   4274560     ['positional_embedding_3[0][0]', \n",
            " rmerDecoder)                                                     'transformer_encoder_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, None, 256)    0           ['transformer_decoder_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, None, 15000)  3855000     ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 17,990,296\n",
            "Trainable params: 17,990,296\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_pred = transformer.predict(train_ds.take(1))[0]"
      ],
      "metadata": {
        "id": "iDkkgBP9qw99"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3fkYZRdq6xJ",
        "outputId": "0b8cd216-26c8-4348-fd9c-e5fbeff491b5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 15000)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=1, validation_data=valid_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlaY3F6KrIV9",
        "outputId": "cda7fe2f-b3cc-4ee1-d329-f94330fdba78"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2104/2104 [==============================] - 5457s 3s/step - loss: 1.5067 - accuracy: 0.4872 - val_loss: 1.1637 - val_accuracy: 0.5802\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83da18aa90>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q-7EH5eZrauS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}