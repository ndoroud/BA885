{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence to sequence models"
      ],
      "metadata": {
        "id": "UjpWClSFsrUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "AXKc-Y7Js6LO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "mxSBTPKZuf3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a language dataset provided by http://www.manythings.org/anki/ which contains Spanish sentences along with their English translations."
      ],
      "metadata": {
        "id": "PQ68WF6VvGsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.get_file('spa-eng.zip',\n",
        "                        'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "J79Nm3Olsu5j",
        "outputId": "40aa8bad-40f9-4947-be57-d36ca0c40059"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./datasets/spa-eng.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"datasets/spa-eng/spa.txt\"\n",
        "with open(text_file) as text:\n",
        "    lines = text.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "ixz3rwLFteuS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs[310:315]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbPHhPOIt7rr",
        "outputId": "ee5b779d-1d02-4270-d067-b4ea3a93c628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"I'm free!\", '[start] ¡Soy libre! [end]'),\n",
              " (\"I'm free.\", '[start] Yo soy libre. [end]'),\n",
              " (\"I'm full.\", '[start] Estoy lleno. [end]'),\n",
              " (\"I'm full.\", '[start] Estoy llena. [end]'),\n",
              " (\"I'm full.\", '[start] Ya me llené. [end]')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_parirs = shuffle(text_pairs)\n",
        "\n",
        "num_train = int(0.7 * len(text_pairs))\n",
        "num_valid = int(0.2 * len(text_pairs))\n",
        "\n",
        "train_pairs = text_pairs[:num_train]\n",
        "valid_pairs = text_pairs[num_train:num_train + num_valid]\n",
        "test_pairs = text_pairs[num_train + num_valid:]"
      ],
      "metadata": {
        "id": "MrkJWDGHxO1j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "I9ElURlDv_84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardization & vectorization"
      ],
      "metadata": {
        "id": "3AC2hUIwwGsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    # target sentences are longer since they start with [start].\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "kToH53RQuEK5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "valid_ds = make_dataset(valid_pairs)"
      ],
      "metadata": {
        "id": "sn3OmUCJzXBg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL9Zrrew0GwL",
        "outputId": "73689365-4bbc-43aa-f5aa-c14a89b70694"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence to sequence model"
      ],
      "metadata": {
        "id": "AmJhsJWs0OZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "daC7dk1e0SqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "encoder_input = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(encoder_input)\n",
        "encoder_output = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "CC9DeSvF0HC_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "rGIFJb290y3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(decoder_input)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoder_output)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "next_target = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model(inputs = [encoder_input, decoder_input], \n",
        "                          outputs = next_target)"
      ],
      "metadata": {
        "id": "bbDtEonc00Je"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7oentjZCBkK",
        "outputId": "b5a23245-bd8d-4b6d-cfcf-fb7ce4fcb0b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " english (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " spanish (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    3840000     ['english[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    3840000     ['spanish[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 1024)         7876608     ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " gru_1 (GRU)                    (None, None, 1024)   3938304     ['embedding_1[0][0]',            \n",
            "                                                                  'bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, None, 1024)   0           ['gru_1[0][0]']                  \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 15000)  15375000    ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 34,869,912\n",
            "Trainable params: 34,869,912\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=4, validation_data=valid_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlNknUiZ1fkm",
        "outputId": "55f3ec1f-b70c-4d5c-c34f-351b70b36076"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1302/1302 [==============================] - 203s 146ms/step - loss: 0.5570 - accuracy: 0.6999 - val_loss: 1.2283 - val_accuracy: 0.5617\n",
            "Epoch 2/4\n",
            "1302/1302 [==============================] - 186s 143ms/step - loss: 0.4111 - accuracy: 0.7500 - val_loss: 1.1896 - val_accuracy: 0.5755\n",
            "Epoch 3/4\n",
            "1302/1302 [==============================] - 186s 143ms/step - loss: 0.3056 - accuracy: 0.7899 - val_loss: 1.2228 - val_accuracy: 0.5782\n",
            "Epoch 4/4\n",
            "1302/1302 [==============================] - 187s 144ms/step - loss: 0.2397 - accuracy: 0.8211 - val_loss: 1.2583 - val_accuracy: 0.5787\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e0e038e90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJUOWZmL7aUx",
        "outputId": "da055638-f4c0-49e2-c94a-5c6bb5bc273d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Tom and Mary were both shocked by what John did.\n",
            "[start] ambos tom y mary estaban [UNK] con john [end]\n",
            "-\n",
            "Judging from his appearance, he may be a soldier.\n",
            "[start] de cuánto ser un poco de su ayuda [end]\n",
            "-\n",
            "One way to lower the number of errors in the Tatoeba Corpus would be to encourage people to only translate into their native languages.\n",
            "[start] a qué hora de la manera [UNK] a ser personas [end]\n",
            "-\n",
            "Workers in France receive four weeks of paid vacation each year.\n",
            "[start] los doce de pie en los niños viven [end]\n",
            "-\n",
            "I took time off from work and went to the doctor.\n",
            "[start] paso al lado de camino al doctor [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W1CmLUXH8GG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}